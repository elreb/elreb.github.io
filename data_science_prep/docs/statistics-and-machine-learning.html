<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Statistics and Machine Learning | Data Science Prep Guide</title>
  <meta name="description" content="Lab practices and data science best practices" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Statistics and Machine Learning | Data Science Prep Guide" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lab practices and data science best practices" />
  <meta name="github-repo" content="elreb/data_science_prep" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Statistics and Machine Learning | Data Science Prep Guide" />
  
  <meta name="twitter:description" content="Lab practices and data science best practices" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="python-r-analyses.html"/>
<link rel="next" href="case-studies.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science Interview Guide</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to the Data Science Prep Guide</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="introduction.html"><a href="introduction.html#sections"><i class="fa fa-check"></i><b>1.0.1</b> Sections</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sql-questions.html"><a href="sql-questions.html"><i class="fa fa-check"></i><b>2</b> SQL questions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sql-questions.html"><a href="sql-questions.html#sql-tips"><i class="fa fa-check"></i><b>2.1</b> SQL tips</a></li>
<li class="chapter" data-level="2.2" data-path="sql-questions.html"><a href="sql-questions.html#sample-questions"><i class="fa fa-check"></i><b>2.2</b> Sample Questions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="sql-questions.html"><a href="sql-questions.html#calculate-the-30-day-readmission-rate"><i class="fa fa-check"></i><b>2.2.1</b> Calculate the 30 day readmission rate</a></li>
<li class="chapter" data-level="2.2.2" data-path="sql-questions.html"><a href="sql-questions.html#top-10-diagnoses-from-encounters-with-a-length-of-service-greater-then-3-days"><i class="fa fa-check"></i><b>2.2.2</b> Top 10 diagnoses from encounters with a length of service greater then 3 days</a></li>
<li class="chapter" data-level="2.2.3" data-path="sql-questions.html"><a href="sql-questions.html#employee-survey-results"><i class="fa fa-check"></i><b>2.2.3</b> Employee survey results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="python-r-analyses.html"><a href="python-r-analyses.html"><i class="fa fa-check"></i><b>3</b> Python / R analyses</a>
<ul>
<li class="chapter" data-level="3.1" data-path="python-r-analyses.html"><a href="python-r-analyses.html#sample-questions-1"><i class="fa fa-check"></i><b>3.1</b> Sample Questions</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="python-r-analyses.html"><a href="python-r-analyses.html#active-users-on-a-messaging-application"><i class="fa fa-check"></i><b>3.1.1</b> Active users on a messaging application</a></li>
<li class="chapter" data-level="3.1.2" data-path="python-r-analyses.html"><a href="python-r-analyses.html#time-for-a-response-on-a-messaging-application"><i class="fa fa-check"></i><b>3.1.2</b> Time for a response on a messaging application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html"><i class="fa fa-check"></i><b>4</b> Statistics and Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#overview-of-important-concepts"><i class="fa fa-check"></i><b>4.1</b> Overview of Important Concepts</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#probability-and-sampling-distributions"><i class="fa fa-check"></i><b>4.1.1</b> Probability and Sampling Distributions</a></li>
<li class="chapter" data-level="4.1.2" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.1.2</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.1.3" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#statistical-experiments-and-significance-testing"><i class="fa fa-check"></i><b>4.1.3</b> Statistical Experiments and Significance Testing</a></li>
<li class="chapter" data-level="4.1.4" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#regression-and-classification"><i class="fa fa-check"></i><b>4.1.4</b> Regression and Classification</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#sample-questions-2"><i class="fa fa-check"></i><b>4.2</b> Sample Questions</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#comparison-of-means"><i class="fa fa-check"></i><b>4.2.1</b> Comparison of means</a></li>
<li class="chapter" data-level="4.2.2" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#principle-of-inclusionexclusion"><i class="fa fa-check"></i><b>4.2.2</b> Principle of Inclusion/Exclusion</a></li>
<li class="chapter" data-level="4.2.3" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#bayes-theorem-1"><i class="fa fa-check"></i><b>4.2.3</b> Bayes Theorem 1</a></li>
<li class="chapter" data-level="4.2.4" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#mae-vs-mse"><i class="fa fa-check"></i><b>4.2.4</b> MAE vs MSE</a></li>
<li class="chapter" data-level="4.2.5" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#bias-variance-tradeoff-1"><i class="fa fa-check"></i><b>4.2.5</b> Bias variance tradeoff</a></li>
<li class="chapter" data-level="4.2.6" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#changing-the-companys-color"><i class="fa fa-check"></i><b>4.2.6</b> Changing the company’s color</a></li>
<li class="chapter" data-level="4.2.7" data-path="statistics-and-machine-learning.html"><a href="statistics-and-machine-learning.html#repeated-significance-testing-error"><i class="fa fa-check"></i><b>4.2.7</b> Repeated significance testing error</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>5</b> Case Studies</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-studies.html"><a href="case-studies.html#sample-questions-3"><i class="fa fa-check"></i><b>5.1</b> Sample Questions</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="case-studies.html"><a href="case-studies.html#ab-testing-basics-1"><i class="fa fa-check"></i><b>5.1.1</b> AB Testing Basics 1</a></li>
<li class="chapter" data-level="5.1.2" data-path="case-studies.html"><a href="case-studies.html#ab-testing-new-ecommerce-recommendation-engine"><i class="fa fa-check"></i><b>5.1.2</b> A/B testing new eCommerce recommendation engine</a></li>
<li class="chapter" data-level="5.1.3" data-path="case-studies.html"><a href="case-studies.html#ab-testing-a-new-landing-page"><i class="fa fa-check"></i><b>5.1.3</b> A/B testing a new landing page</a></li>
<li class="chapter" data-level="5.1.4" data-path="case-studies.html"><a href="case-studies.html#stopping-an-ab-test-early"><i class="fa fa-check"></i><b>5.1.4</b> Stopping an AB Test early</a></li>
<li class="chapter" data-level="5.1.5" data-path="case-studies.html"><a href="case-studies.html#ab-testing-from-sql-to-modeling"><i class="fa fa-check"></i><b>5.1.5</b> AB Testing from SQL to modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="computer-science-data-scructures-and-algorithms.html"><a href="computer-science-data-scructures-and-algorithms.html"><i class="fa fa-check"></i><b>6</b> Computer Science / Data Scructures and Algorithms</a>
<ul>
<li class="chapter" data-level="6.1" data-path="computer-science-data-scructures-and-algorithms.html"><a href="computer-science-data-scructures-and-algorithms.html#sample-questions-4"><i class="fa fa-check"></i><b>6.1</b> Sample Questions</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="computer-science-data-scructures-and-algorithms.html"><a href="computer-science-data-scructures-and-algorithms.html#calulate-moving-average-using-python"><i class="fa fa-check"></i><b>6.1.1</b> Calulate moving average using Python</a></li>
<li class="chapter" data-level="6.1.2" data-path="computer-science-data-scructures-and-algorithms.html"><a href="computer-science-data-scructures-and-algorithms.html#python-function-to-express-power-sets"><i class="fa fa-check"></i><b>6.1.2</b> Python function to express power sets</a></li>
<li class="chapter" data-level="6.1.3" data-path="computer-science-data-scructures-and-algorithms.html"><a href="computer-science-data-scructures-and-algorithms.html#if-you-were-given-a-m-by-n-matrix-how-would-you-find-the-minimum-element-using-brute-force-algorithm"><i class="fa fa-check"></i><b>6.1.3</b> If you were given a m by n matrix how would you find the minimum element using brute force algorithm</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/elreb/data_science_prep" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Prep Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics-and-machine-learning" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Statistics and Machine Learning<a href="statistics-and-machine-learning.html#statistics-and-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="overview-of-important-concepts" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Overview of Important Concepts<a href="statistics-and-machine-learning.html#overview-of-important-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="probability-and-sampling-distributions" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Probability and Sampling Distributions<a href="statistics-and-machine-learning.html#probability-and-sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below is a review of conditional probabilities, Bayes’ theorem, and central limit theorem and how to handle questions that work with commonly referenced probability distributions <a href="https://campus.datacamp.com/courses/practicing-statistics-interview-questions-in-python/probability-and-sampling-distributions?ex=1">source</a></p>
<div id="conditional-probabilities-bayes-theorem" class="section level4 hasAnchor" number="4.1.1.1">
<h4><span class="header-section-number">4.1.1.1</span> Conditional Probabilities &amp; Bayes Theorem<a href="statistics-and-machine-learning.html#conditional-probabilities-bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In conditional probabilities, we want to figure out the probability of something happening, given that we have some additional information that may influence the outcome.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="assets/images/venn_intersection.png" alt="There is an overlap between sets A and B, which represents the intersection of both sets (and the probability of both independent events occurring)." width="40%" />
<p class="caption">
Figure 4.1: There is an overlap between sets A and B, which represents the intersection of both sets (and the probability of both independent events occurring).
</p>
</div>
<p><strong>BAYES THEOREM</strong></p>
<p>Bayes theorem is a staple in Data Science interviews. Bayes’ theorem helps us tackle probability questions where we already know about the probability of B given A, but we want to find the probability of A given B.</p>
<p><img src="assets/images/bayes_theorem_on_wall.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Above we are solving for A given B by multiplying the independent events in the numerator to get the probability of A and B occurring together. We then divide the by probability of B to get the answer.</p>
<p>Another way to solve these questions is through tree diagrams. Since given a sequence of independent events, you can chain together the singular probabilities to compute the overall probability.</p>
<p><em>Example 1 :</em></p>
<p><code>What is the probability that the applicant passes the stats interview, given that he or she passes the coding interview as well?</code></p>
<ol style="list-style-type: decimal">
<li>make tree diagram</li>
<li>multiply the independent events then compute the probability of each outcome</li>
<li>plug into Bayes theorem</li>
</ol>
<p><img src="assets/images/bayes_tree_example.png" width="30%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="statistics-and-machine-learning.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#passes both the stats and coding interviews</span></span>
<span id="cb14-2"><a href="statistics-and-machine-learning.html#cb14-2" aria-hidden="true" tabindex="-1"></a>both <span class="op">=</span> <span class="fl">0.25</span> <span class="op">*</span> <span class="fl">0.40</span></span>
<span id="cb14-3"><a href="statistics-and-machine-learning.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">#0.1</span></span>
<span id="cb14-4"><a href="statistics-and-machine-learning.html#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="statistics-and-machine-learning.html#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">#fails stats and passes the coding interview</span></span>
<span id="cb14-6"><a href="statistics-and-machine-learning.html#cb14-6" aria-hidden="true" tabindex="-1"></a>coding <span class="op">=</span> (<span class="fl">0.25</span> <span class="op">*</span> <span class="fl">0.40</span>) <span class="op">+</span> (<span class="fl">0.75</span> <span class="op">*</span> <span class="fl">0.20</span>)</span>
<span id="cb14-7"><a href="statistics-and-machine-learning.html#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">#0.25</span></span>
<span id="cb14-8"><a href="statistics-and-machine-learning.html#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="statistics-and-machine-learning.html#cb14-9" aria-hidden="true" tabindex="-1"></a>stats_given_coding <span class="op">=</span> both <span class="op">/</span> coding </span>
<span id="cb14-10"><a href="statistics-and-machine-learning.html#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(stats_given_coding)</span></code></pre></div>
<pre><code>## 0.4</code></pre>
<ul>
<li>Therefore, there is a 40% chance of passing the the stats interview, given that they passed the coding interview.</li>
</ul>
<p><em>Example 2:</em></p>
<p>You have two coins in your hand. Out of the two coins, one is a real coin (heads and tails) and the other is a faulty coin with tails on both sides.</p>
<p>You are blindfolded and forced to choose a random coin and then toss it in the air. The coin lands with tails facing upwards. Find the probability that this is the faulty coin.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="statistics-and-machine-learning.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># (tails)</span></span>
<span id="cb16-2"><a href="statistics-and-machine-learning.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># P(faulty)</span></span>
<span id="cb16-3"><a href="statistics-and-machine-learning.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># P(tails and faulty)</span></span>
<span id="cb16-4"><a href="statistics-and-machine-learning.html#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print P(faulty | tails)</span></span>
<span id="cb16-5"><a href="statistics-and-machine-learning.html#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="dv">3</span> <span class="op">/</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## 0.75</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="statistics-and-machine-learning.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## 0.5</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="statistics-and-machine-learning.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span>) <span class="op">*</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 0.5</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="statistics-and-machine-learning.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((<span class="fl">0.5</span><span class="op">*</span><span class="dv">1</span>)<span class="op">/</span>(<span class="fl">0.5</span><span class="op">*</span><span class="fl">0.5</span><span class="op">+</span><span class="fl">0.5</span><span class="op">*</span><span class="dv">1</span>))</span></code></pre></div>
<pre><code>## 0.6666666666666666</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-15"></span>
<img src="assets/images/bayes_coin_ex.jpeg" alt="tree diagram for faultly coin" width="45%" />
<p class="caption">
Figure 4.2: tree diagram for faultly coin
</p>
</div>
</div>
<div id="central-limit-theorem" class="section level4 hasAnchor" number="4.1.1.2">
<h4><span class="header-section-number">4.1.1.2</span> Central limit theorem<a href="statistics-and-machine-learning.html#central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>What</strong> - Central limit theorem says that with a large enough collection of samples from the same population, the sample means will be normally distributed. Note that this doesn’t make any assumptions about the underlying distribution of the data; with a reasonably large sample of roughly 30 or more, this theorem will always ring true no matter what the population looks like.</p></li>
<li><p><strong>Why</strong> - Central limit theorem matters because it promises our sampling mean distribution will be normal, therefore we can perform hypothesis tests. More concretely, we can assess the likelihood that a given mean came from a particular distribution and then, based on this, reject or fail to reject our hypothesis. This empowers all of the A/B testing you see in practice.</p></li>
<li><p><strong>CLT vs law of large numbers</strong> - The law of large numbers states that as the size of a sample is increased, the estimate of the sample mean will more accurately reflect the population mean. This is different from the central limit theorem which is more broadly about normality vs sampling.</p></li>
</ul>
<p><strong>law of large numbers example:</strong></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="statistics-and-machine-learning.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> randint</span>
<span id="cb24-2"><a href="statistics-and-machine-learning.html#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co">#randint is exclusive for high number (second parameter)</span></span>
<span id="cb24-3"><a href="statistics-and-machine-learning.html#cb24-3" aria-hidden="true" tabindex="-1"></a>small <span class="op">=</span> randint(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">10</span>)</span>
<span id="cb24-4"><a href="statistics-and-machine-learning.html#cb24-4" aria-hidden="true" tabindex="-1"></a>small_mean <span class="op">=</span> <span class="bu">sum</span>(small) <span class="op">/</span> <span class="bu">len</span>(small)</span>
<span id="cb24-5"><a href="statistics-and-machine-learning.html#cb24-5" aria-hidden="true" tabindex="-1"></a>large <span class="op">=</span> randint(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">1000</span>)</span>
<span id="cb24-6"><a href="statistics-and-machine-learning.html#cb24-6" aria-hidden="true" tabindex="-1"></a>large_mean <span class="op">=</span> <span class="bu">sum</span>(large) <span class="op">/</span> <span class="bu">len</span>(large)</span>
<span id="cb24-7"><a href="statistics-and-machine-learning.html#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(small_mean)</span></code></pre></div>
<pre><code>## 3.9</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="statistics-and-machine-learning.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(large_mean)</span></code></pre></div>
<pre><code>## 3.553</code></pre>
<ul>
<li>Notice how the mean of the large sample has gotten closer to the true expected mean value of 3.5 for a rolled die.</li>
</ul>
<p><strong>CLT example:</strong></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="statistics-and-machine-learning.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> randint</span>
<span id="cb28-2"><a href="statistics-and-machine-learning.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-3"><a href="statistics-and-machine-learning.html#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># generating samples</span></span>
<span id="cb28-4"><a href="statistics-and-machine-learning.html#cb28-4" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [randint(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">30</span>).mean() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)]</span>
<span id="cb28-5"><a href="statistics-and-machine-learning.html#cb28-5" aria-hidden="true" tabindex="-1"></a>plt.hist(means)</span></code></pre></div>
<pre><code>## (array([ 4.,  6., 14., 22., 16., 18., 13.,  3.,  3.,  1.]), array([2.8       , 2.96666667, 3.13333333, 3.3       , 3.46666667,
##        3.63333333, 3.8       , 3.96666667, 4.13333333, 4.3       ,
##        4.46666667]), &lt;a list of 10 Patch objects&gt;)</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="statistics-and-machine-learning.html#cb30-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-17-1.png" width="288" /></p>
</div>
<div id="probabilty-distributions" class="section level4 hasAnchor" number="4.1.1.3">
<h4><span class="header-section-number">4.1.1.3</span> Probabilty distributions<a href="statistics-and-machine-learning.html#probabilty-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Probability distributions are to statistics what data structures are to computer science.</p>
<ul>
<li><em>Simple description</em> - they indicate the likelihood of an outcome.</li>
<li><em>Properties</em>
<ul>
<li>probabilities must add up to 1
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-18"></span>
<img src="assets/images/generic_probabilty_dist.png" alt="Area under the curve (AUC) equals 1." width="40%" />
<p class="caption">
Figure 4.3: Area under the curve (AUC) equals 1.
</p>
</div></li>
</ul></li>
<li><em>Types</em>
<ol style="list-style-type: decimal">
<li><strong>Bernoulli</strong></li>
<li><strong>Binomial</strong></li>
<li><strong>Poisson</strong></li>
<li><strong>Normal(Gaussian)</strong></li>
<li>Others: Uniform, hypergeometric, log normal, student’s t, chi-squared, gamma, beta, webull, exponential, geometric, negative binomial</li>
</ol></li>
</ul>
<div id="bernoulli-distribution" class="section level5 hasAnchor" number="4.1.1.3.1">
<h5><span class="header-section-number">4.1.1.3.1</span> Bernoulli distribution<a href="statistics-and-machine-learning.html#bernoulli-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Bernoulli is a discrete distribution that models the probability of two outcomes (e.g. a coin flip). There only two possible outcomes, and the probability of one is always <code>1- &lt;the_other&gt;</code>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="statistics-and-machine-learning.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co">#simulating bernoulli data with scipy stats</span></span>
<span id="cb31-2"><a href="statistics-and-machine-learning.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli</span>
<span id="cb31-3"><a href="statistics-and-machine-learning.html#cb31-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> bernoulli.rvs(p<span class="op">=</span><span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb31-4"><a href="statistics-and-machine-learning.html#cb31-4" aria-hidden="true" tabindex="-1"></a>plt.hist(data)</span></code></pre></div>
<pre><code>## (array([473.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 527.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), &lt;a list of 10 Patch objects&gt;)</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="statistics-and-machine-learning.html#cb33-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-19-1.png" width="288" /></p>
<p>When using a small sample size heads and tails don’t have the exact same probability. This is no fluke — when sampling, we won’t always get perfect results. We can increase our accuracy however, when you increase the size of the sample.</p>
</div>
<div id="binomial-distribution" class="section level5 hasAnchor" number="4.1.1.3.2">
<h5><span class="header-section-number">4.1.1.3.2</span> Binomial distribution<a href="statistics-and-machine-learning.html#binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The Binomial distribution can be thought of as the sum of the outcomes of multiple Bernoulli trials, meaning those that have an established success and failure. It’s used to model the number of successful outcomes in trials where there is some consistent probability of success.</p>
<ul>
<li><em>Parameters</em>
<ul>
<li><code>k</code> - number of successes</li>
<li><code>n</code> - number of trials</li>
<li><code>p</code> - probability of success</li>
</ul></li>
</ul>
<p><strong>Motivating example:</strong> Consider a game where you are trying to make a ball in a basket. You are given 10 shots and you know that you have an 80% chance of making a given shot.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="statistics-and-machine-learning.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">#simulating binomial data with scipy stats</span></span>
<span id="cb34-2"><a href="statistics-and-machine-learning.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb34-3"><a href="statistics-and-machine-learning.html#cb34-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> binom.rvs(n<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span><span class="fl">0.80</span>, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb34-4"><a href="statistics-and-machine-learning.html#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.hist(data)</span></code></pre></div>
<pre><code>## (array([  6.,  29.,   0.,  80.,   0., 211., 297.,   0., 263., 114.]), array([ 4. ,  4.6,  5.2,  5.8,  6.4,  7. ,  7.6,  8.2,  8.8,  9.4, 10. ]), &lt;a list of 10 Patch objects&gt;)</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="statistics-and-machine-learning.html#cb36-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-20-3.png" width="288" /></p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="statistics-and-machine-learning.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability of making 8 or less shots out of 10</span></span>
<span id="cb37-2"><a href="statistics-and-machine-learning.html#cb37-2" aria-hidden="true" tabindex="-1"></a>prob1 <span class="op">=</span> binom.cdf(k<span class="op">=</span><span class="dv">8</span>, n<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb37-3"><a href="statistics-and-machine-learning.html#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability of making 10 out of 10 shots</span></span>
<span id="cb37-4"><a href="statistics-and-machine-learning.html#cb37-4" aria-hidden="true" tabindex="-1"></a>prob2 <span class="op">=</span> binom.pmf(k<span class="op">=</span><span class="dv">10</span>, n<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb37-5"><a href="statistics-and-machine-learning.html#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prob1)</span></code></pre></div>
<pre><code>## 0.6241903616</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="statistics-and-machine-learning.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prob2)</span></code></pre></div>
<pre><code>## 0.10737418240000005</code></pre>
<p>Remember, interviewers like to start out with fundamental concepts before getting incrementally more complex. Above it started with just showing the general shape of the distribution but went into application.</p>
</div>
<div id="normal-gaussian-distribution" class="section level5 hasAnchor" number="4.1.1.3.3">
<h5><span class="header-section-number">4.1.1.3.3</span> Normal (Gaussian) distribution<a href="statistics-and-machine-learning.html#normal-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
The normal distribution is a bell-curve shaped continuous probability distribution that is fundamental to many statistical concepts, like sampling and hypothesis testing.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="assets/images/generic_normal_dist.png" alt="68-95-99.7 rule, aka 68% of observations fall within 1 std, 95% of observations fall within 2 std and so on." width="30%" />
<p class="caption">
Figure 4.4: 68-95-99.7 rule, aka 68% of observations fall within 1 std, 95% of observations fall within 2 std and so on.
</p>
</div>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="statistics-and-machine-learning.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate normal data</span></span>
<span id="cb41-2"><a href="statistics-and-machine-learning.html#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb41-3"><a href="statistics-and-machine-learning.html#cb41-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> norm.rvs(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb41-4"><a href="statistics-and-machine-learning.html#cb41-4" aria-hidden="true" tabindex="-1"></a>plt.hist(data)</span></code></pre></div>
<pre><code>## (array([  9.,  30., 101., 174., 257., 234., 137.,  42.,  11.,   5.]), array([-3.14433248, -2.47368438, -1.80303628, -1.13238819, -0.46174009,
##         0.20890801,  0.8795561 ,  1.5502042 ,  2.2208523 ,  2.89150039,
##         3.56214849]), &lt;a list of 10 Patch objects&gt;)</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="statistics-and-machine-learning.html#cb43-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-23-1.png" width="288" /></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="statistics-and-machine-learning.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Given a standardized normal distribution, what is the probability </span></span>
<span id="cb44-2"><a href="statistics-and-machine-learning.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co">#of an observation greater than 2?</span></span>
<span id="cb44-3"><a href="statistics-and-machine-learning.html#cb44-3" aria-hidden="true" tabindex="-1"></a>true_prob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> norm.cdf(<span class="dv">2</span>)</span>
<span id="cb44-4"><a href="statistics-and-machine-learning.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Looking at our sample, what is the probability of an observation greater than 2?</span></span>
<span id="cb44-5"><a href="statistics-and-machine-learning.html#cb44-5" aria-hidden="true" tabindex="-1"></a>sample_prob <span class="op">=</span> <span class="bu">sum</span>(obs <span class="op">&gt;</span> <span class="dv">2</span> <span class="cf">for</span> obs <span class="kw">in</span> data) <span class="op">/</span> <span class="bu">len</span>(data)</span>
<span id="cb44-6"><a href="statistics-and-machine-learning.html#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="statistics-and-machine-learning.html#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(true_prob)</span></code></pre></div>
<pre><code>## 0.02275013194817921</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="statistics-and-machine-learning.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sample_prob)</span></code></pre></div>
<pre><code>## 0.019</code></pre>
<p>Note that the results from the true distribution and sample distribution are different.</p>
</div>
<div id="poisson-distribution" class="section level5 hasAnchor" number="4.1.1.3.4">
<h5><span class="header-section-number">4.1.1.3.4</span> Poisson distribution<a href="statistics-and-machine-learning.html#poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The poisson distribution represents a count or the number of times something happened. Unlike the binomial distribution, it’s calculated by an average rate (<code>lambda</code>) instead of a probability <code>p</code> and number of trials <code>n</code>. As the rate of events change the distribution changes as well.</p>
<ul>
<li><em>When to use:</em> use the Poisson when counting events over time given some continuous rate.</li>
<li><em>E.g.:</em> In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour?</li>
</ul>
</div>
</div>
</div>
<div id="exploratory-data-analysis" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Exploratory Data Analysis<a href="statistics-and-machine-learning.html#exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="descriptive-statistics" class="section level4 hasAnchor" number="4.1.2.1">
<h4><span class="header-section-number">4.1.2.1</span> Descriptive Statistics<a href="statistics-and-machine-learning.html#descriptive-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>There are two most common questions types in interviews along with some other concepts that don’t go in those categories:</p>
<ol style="list-style-type: decimal">
<li>measures of centrality</li>
</ol>
<ul>
<li><p>Core measures</p>
<ul>
<li><em>mean</em> - average (sum / # of observations)</li>
<li><em>median</em> - middle value when all observations are sorted</li>
<li><em>mode</em> - most common observation, or peak of the distribution
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-25"></span>
<img src="assets/images/mean_median_mode.png" alt="Mean, median and mode." width="40%" />
<p class="caption">
Figure 4.5: Mean, median and mode.
</p>
</div></li>
</ul></li>
<li><p>Notes:</p>
<ul>
<li>Mean, median and mode are all <strong>equal</strong> if the distribution is perfectly normal</li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Measures of variability</li>
</ol>
<ul>
<li>Core measures
<ul>
<li><em>variance</em> - how far spread out your data points are from each other.</li>
<li><em>standard deviation</em> - how far away your data is from the average.</li>
<li><em>range</em> - max - min
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="assets/images/variance_std_formula.png" alt="Formula for variance and standard deviation" width="30%" />
<p class="caption">
Figure 4.6: Formula for variance and standard deviation
</p>
</div></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="statistics-and-machine-learning.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb48-2"><a href="statistics-and-machine-learning.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb48-3"><a href="statistics-and-machine-learning.html#cb48-3" aria-hidden="true" tabindex="-1"></a>nums <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb48-4"><a href="statistics-and-machine-learning.html#cb48-4" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> <span class="bu">sum</span>(nums) <span class="op">/</span> <span class="bu">len</span>(nums)</span>
<span id="cb48-5"><a href="statistics-and-machine-learning.html#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the variance and print the std of the list</span></span>
<span id="cb48-6"><a href="statistics-and-machine-learning.html#cb48-6" aria-hidden="true" tabindex="-1"></a>variance <span class="op">=</span> <span class="bu">sum</span>(<span class="bu">pow</span>(x <span class="op">-</span> mean, <span class="dv">2</span>) <span class="cf">for</span> x <span class="kw">in</span> nums) <span class="op">/</span> <span class="bu">len</span>(nums)</span>
<span id="cb48-7"><a href="statistics-and-machine-learning.html#cb48-7" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> math.sqrt(variance)</span>
<span id="cb48-8"><a href="statistics-and-machine-learning.html#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and print the actual result from numpy</span></span>
<span id="cb48-9"><a href="statistics-and-machine-learning.html#cb48-9" aria-hidden="true" tabindex="-1"></a>real_std <span class="op">=</span> np.array(nums).std()</span>
<span id="cb48-10"><a href="statistics-and-machine-learning.html#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(std)</span></code></pre></div>
<pre><code>## 1.4142135623730951</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="statistics-and-machine-learning.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(real_std)</span></code></pre></div>
<pre><code>## 1.4142135623730951</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Other</li>
</ol>
<ul>
<li>Modality - multiple peaks that show in the data (bi-modal = two peaks)</li>
<li>Skewness - the symmetry of the distribution (skewness is determined by where the the tail is)
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="assets/images/left_skewed_data.png" alt="Left skewed data" width="30%" />
<p class="caption">
Figure 4.7: Left skewed data
</p>
</div></li>
</ul>
</div>
<div id="categorical-data" class="section level4 hasAnchor" number="4.1.2.2">
<h4><span class="header-section-number">4.1.2.2</span> Categorical Data<a href="statistics-and-machine-learning.html#categorical-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Types of categorical data:</em></p>
<ol style="list-style-type: decimal">
<li>ordinal - takes on order (e.g. 5 star movie reviews)</li>
<li>nominal - no order (e.g. gender, or eye color)</li>
</ol>
<p><em>Preprocessing categorical variables</em></p>
<ul>
<li>unlike continuous data, to use machine learning on categorical data you need to encode it.</li>
<li>Types of encoding
<ol style="list-style-type: decimal">
<li>Label encoding - simply assigning a number to the text factor of the category</li>
<li>One Hot encoding - break every subcategory into its own Boolean column</li>
</ol></li>
</ul>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="statistics-and-machine-learning.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb52-2"><a href="statistics-and-machine-learning.html#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="statistics-and-machine-learning.html#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co">#What data looks like:</span></span>
<span id="cb52-4"><a href="statistics-and-machine-learning.html#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(laptops[:<span class="dv">5</span>].to_numpy().tolist())</span>
<span id="cb52-5"><a href="statistics-and-machine-learning.html#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="statistics-and-machine-learning.html#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Encoder - check out new values</span></span></code></pre></div>
<pre><code>## [[&#39;Acer&#39;, &#39;Aspire 3&#39;, 400.0], [&#39;Asus&#39;, &#39;Vivobook E200HA&#39;, 191.9], [&#39;Asus&#39;, &#39;E402WA-GA010T (E2-6110/2GB/32GB/W10)&#39;, 199.0], [&#39;Asus&#39;, &#39;X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux)&#39;, 389.0], [&#39;Asus&#39;, &#39;X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce&#39;, 522.99]]</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="statistics-and-machine-learning.html#cb54-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> preprocessing.LabelEncoder()</span>
<span id="cb54-2"><a href="statistics-and-machine-learning.html#cb54-2" aria-hidden="true" tabindex="-1"></a>company_column_encoded <span class="op">=</span> encoder.fit_transform(laptops[<span class="st">&#39;Company&#39;</span>])</span>
<span id="cb54-3"><a href="statistics-and-machine-learning.html#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(company_column_encoded)</span></code></pre></div>
<pre><code>## [0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 2 1
##  1 0 1 2 1 0 0 0 0 2 0 2 0 0 0 1 2 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 2 0 0
##  0 1 0 1 0 1 1 1 2 0 1 0 1 0 0 1 2 1 1 1 1 2 1 1 0 1]</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="statistics-and-machine-learning.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode Company for laptops2</span></span>
<span id="cb56-2"><a href="statistics-and-machine-learning.html#cb56-2" aria-hidden="true" tabindex="-1"></a>laptops2 <span class="op">=</span> pd.get_dummies(data<span class="op">=</span>laptops, columns<span class="op">=</span>[<span class="st">&#39;Company&#39;</span>])</span>
<span id="cb56-3"><a href="statistics-and-machine-learning.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(laptops2.info())</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
## RangeIndex: 100 entries, 0 to 99
## Data columns (total 5 columns):
## Product            100 non-null object
## Price              100 non-null float64
## Company_Acer       100 non-null uint8
## Company_Asus       100 non-null uint8
## Company_Toshiba    100 non-null uint8
## dtypes: float64(1), object(1), uint8(3)
## memory usage: 2.0+ KB
## None</code></pre>
<ul>
<li>Be careful about the high dimensionality of using one-hot-encoding!</li>
</ul>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="statistics-and-machine-learning.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb58-2"><a href="statistics-and-machine-learning.html#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb58-3"><a href="statistics-and-machine-learning.html#cb58-3" aria-hidden="true" tabindex="-1"></a>sns.countplot(laptops[<span class="st">&#39;Company&#39;</span>])</span>
<span id="cb58-4"><a href="statistics-and-machine-learning.html#cb58-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb58-5"><a href="statistics-and-machine-learning.html#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="statistics-and-machine-learning.html#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the relationship with price</span></span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-32-1.png" width="288" /></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="statistics-and-machine-learning.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb59-2"><a href="statistics-and-machine-learning.html#cb59-2" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">80</span>)</span>
<span id="cb59-3"><a href="statistics-and-machine-learning.html#cb59-3" aria-hidden="true" tabindex="-1"></a>laptops.boxplot(<span class="st">&#39;Price&#39;</span>, <span class="st">&#39;Company&#39;</span>,rot<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb59-4"><a href="statistics-and-machine-learning.html#cb59-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
</div>
<div id="two-or-more-variables" class="section level4 hasAnchor" number="4.1.2.3">
<h4><span class="header-section-number">4.1.2.3</span> Two or more variables<a href="statistics-and-machine-learning.html#two-or-more-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Overview:</strong> Comparing the relationships between two or more numerical variables including correlation, confidence intervals and more.</p>
<ol style="list-style-type: decimal">
<li>Correlation - describes the relatedness between variables, meaning how much information variables reveal about each other.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-33"></span>
<img src="assets/images/scatter_plot_correlation.png" alt="Correlation with various scatterplots" width="30%" />
<p class="caption">
Figure 4.8: Correlation with various scatterplots
</p>
</div></li>
<li>Covariance - the average of the product between the values of each sample where the values have each had their mean subtracted.
<img src="assets/images/covariance_formula.png" width="25%" style="display: block; margin: auto;" />
<ul>
<li>difficult to interpret since don’t get a magnitude, but we can use it to get the <em>Pearsons correlation coefficient</em></li>
</ul></li>
<li>Pearson’s correlation coefficent - denoted as lowercase <code>r</code>, is the covariance function divided by the product of the sample standard deviations of each variable.
<img src="assets/images/pearsons_correlation_co_formula.png" width="25%" style="display: block; margin: auto;" />
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-36"></span>
<img src="assets/images/pearsons_correlation_co_gen_examples.png" alt="Positive r means there is a positive relationship, and vise versa. 1 (or -1) is perfect, 0 is no corelation." width="40%" />
<p class="caption">
Figure 4.9: Positive r means there is a positive relationship, and vise versa. 1 (or -1) is perfect, 0 is no corelation.
</p>
</div></li>
<li>R^2 - is simply Pearson’s correlation coefficent squared. Interpreted as the amount of variable Y that is explained by X.</li>
</ol>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="statistics-and-machine-learning.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data import hidden - obviously</span></span>
<span id="cb60-2"><a href="statistics-and-machine-learning.html#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb60-3"><a href="statistics-and-machine-learning.html#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb60-4"><a href="statistics-and-machine-learning.html#cb60-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-5"><a href="statistics-and-machine-learning.html#cb60-5" aria-hidden="true" tabindex="-1"></a>sns.pairplot(weather)</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-38-1.png" width="495" /></p>
<ul>
<li>Some bivariate comparisons appear more correlated then others.</li>
</ul>
<p><strong>Lets look at <code>Humidity9am</code> and <code>Humidity3pm</code> closer:</strong></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="statistics-and-machine-learning.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb61-2"><a href="statistics-and-machine-learning.html#cb61-2" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">80</span>)</span>
<span id="cb61-3"><a href="statistics-and-machine-learning.html#cb61-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(weather[<span class="st">&#39;Humidity9am&#39;</span>], weather[<span class="st">&#39;Humidity3pm&#39;</span>])</span>
<span id="cb61-4"><a href="statistics-and-machine-learning.html#cb61-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-39-3.png" width="288" /></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="statistics-and-machine-learning.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># corr is built into pandas</span></span>
<span id="cb62-2"><a href="statistics-and-machine-learning.html#cb62-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> weather[<span class="st">&#39;Humidity9am&#39;</span>].corr(weather[<span class="st">&#39;Humidity3pm&#39;</span>])</span>
<span id="cb62-3"><a href="statistics-and-machine-learning.html#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="statistics-and-machine-learning.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the r-squared value and print the result</span></span>
<span id="cb62-5"><a href="statistics-and-machine-learning.html#cb62-5" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r <span class="op">*</span> r</span>
<span id="cb62-6"><a href="statistics-and-machine-learning.html#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(r2)</span></code></pre></div>
<pre><code>## 0.446620063530763</code></pre>
<ul>
<li>We see here that humidity in the morning has a moderately strong correlation with afternoon humidity, giving us a ~0.67 pearson coefficient. When we square that result, we get a r-squared value of ~0.45, meaning that <code>Humidity9am</code> explains around 45% of the variability in the <code>Humidity3pm</code> variable.</li>
</ul>
<p><strong>Not so robust to outliers</strong>
When outliers are present the metric is not very robust. So often you must seek other tools when the outliers can’t or shouldn’t be easily removed.</p>
<p><em>Alternatives:</em></p>
<ol style="list-style-type: decimal">
<li>Spearman’s/Kendall</li>
</ol>
<p>Things to consider before removing an outlier:</p>
<ul>
<li>If you’re actually building a predictive model, do you have additional data on which to test your model for predictive power?
<ul>
<li>If yes, then you can see what happens when you remove this value.</li>
</ul></li>
<li>Does removing it improve your ability to make predictions on new data? Then maybe it really is an outlier. Or is it actually providing some important information on the range of possible but perhaps rare outcomes?</li>
<li>Would it be unreasonable to expect to see these kinds of extreme values in new data in the future? Then maybe it’s better to include it, or consider a model that can better capture these kinds of values. Like a robust regression</li>
</ul>
<p><strong>Correlation vs Causation</strong></p>
<ul>
<li>Correlation ≠ causation.</li>
<li>Need experimentation (<code>sufficiency | necessity</code>)</li>
</ul>
</div>
</div>
<div id="statistical-experiments-and-significance-testing" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Statistical Experiments and Significance Testing<a href="statistics-and-machine-learning.html#statistical-experiments-and-significance-testing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Introduction into sampling</strong></p>
<p>A sample is a collection of data from a certain population that is meant to represent the whole. However, sampling only makes up a small proportion of the total population. But with statistics we want to make conclusions about the sample and generalize it to a broader group, also known as <em>inference</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-41"></span>
<img src="assets/images/error_types.jpeg" alt="Error types in statistics" width="40%" />
<p class="caption">
Figure 4.10: Error types in statistics
</p>
</div>
<div id="confiendence-intervals" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Confiendence Intervals<a href="statistics-and-machine-learning.html#confiendence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Confidence interval is a range of values that we are fairly sure includes the true value of an unknown population parameter. It has an associated confidence level that represents the frequency in which the interval will contain this value.</p>
<ul>
<li>E.g. a 95% confidence interval means that 95 times out of 100 we can expect our interval to hold the true parameter value of the population.</li>
</ul>
<p>How to calculate:</p>
<ol style="list-style-type: decimal">
<li>means - take the sample mean and then subtract the appropriate z-score for your confidence level with the population standard deviation over the square root of the number of samples. Takes on a different form if you don’t know the population variance.</li>
</ol>
<p><img src="assets/images/means_confidence_interval.png" width="30%" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li>proportions - take the mean plus minus the z-score times the square root of the proportion times its inverse, over the number of samples.</li>
</ol>
<p><img src="assets/images/proportion_confidence_interval.png" width="30%" style="display: block; margin: auto;" /></p>
<ul>
<li>Both above formulas are alike since they take the mean plus minus some value that is computed. The value is referred to as the margin of error. Adding it to the mean gives us the upper value threshold and subtracting it gives us the lower threshold.</li>
</ul>
<p>How a interviewer will ask you about CI:</p>
<ol style="list-style-type: decimal">
<li>explain in simple terms</li>
<li>elaborate on how they are calculated, and maybe implement it.</li>
</ol>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="statistics-and-machine-learning.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> sem, t</span>
<span id="cb64-2"><a href="statistics-and-machine-learning.html#cb64-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb64-3"><a href="statistics-and-machine-learning.html#cb64-3" aria-hidden="true" tabindex="-1"></a>confidence <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb64-4"><a href="statistics-and-machine-learning.html#cb64-4" aria-hidden="true" tabindex="-1"></a>z_score <span class="op">=</span> <span class="fl">2.7764451051977987</span></span>
<span id="cb64-5"><a href="statistics-and-machine-learning.html#cb64-5" aria-hidden="true" tabindex="-1"></a>sample_mean <span class="op">=</span> <span class="fl">3.0</span></span>
<span id="cb64-6"><a href="statistics-and-machine-learning.html#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="statistics-and-machine-learning.html#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the standard error and margin of error</span></span>
<span id="cb64-8"><a href="statistics-and-machine-learning.html#cb64-8" aria-hidden="true" tabindex="-1"></a>std_err <span class="op">=</span> sem(data)</span>
<span id="cb64-9"><a href="statistics-and-machine-learning.html#cb64-9" aria-hidden="true" tabindex="-1"></a>margin_error <span class="op">=</span> std_err <span class="op">*</span> z_score</span>
<span id="cb64-10"><a href="statistics-and-machine-learning.html#cb64-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-11"><a href="statistics-and-machine-learning.html#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and print the lower threshold</span></span>
<span id="cb64-12"><a href="statistics-and-machine-learning.html#cb64-12" aria-hidden="true" tabindex="-1"></a>lower <span class="op">=</span> sample_mean <span class="op">-</span> margin_error</span>
<span id="cb64-13"><a href="statistics-and-machine-learning.html#cb64-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-14"><a href="statistics-and-machine-learning.html#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and print the upper threshold</span></span>
<span id="cb64-15"><a href="statistics-and-machine-learning.html#cb64-15" aria-hidden="true" tabindex="-1"></a>upper <span class="op">=</span> sample_mean <span class="op">+</span> margin_error</span>
<span id="cb64-16"><a href="statistics-and-machine-learning.html#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lower)</span></code></pre></div>
<pre><code>## 1.036756838522439</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="statistics-and-machine-learning.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(upper)</span></code></pre></div>
<pre><code>## 4.9632431614775605</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="statistics-and-machine-learning.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean and std</span></span>
<span id="cb68-2"><a href="statistics-and-machine-learning.html#cb68-2" aria-hidden="true" tabindex="-1"></a>mean, std <span class="op">=</span> laptops[<span class="st">&#39;Price&#39;</span>].mean(), laptops[<span class="st">&#39;Price&#39;</span>].std()</span>
<span id="cb68-3"><a href="statistics-and-machine-learning.html#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="statistics-and-machine-learning.html#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and print the upper and lower threshold</span></span>
<span id="cb68-5"><a href="statistics-and-machine-learning.html#cb68-5" aria-hidden="true" tabindex="-1"></a>cut_off <span class="op">=</span> std <span class="op">*</span> <span class="dv">3</span></span>
<span id="cb68-6"><a href="statistics-and-machine-learning.html#cb68-6" aria-hidden="true" tabindex="-1"></a>lower, upper <span class="op">=</span> mean <span class="op">-</span> cut_off, mean <span class="op">+</span> cut_off</span>
<span id="cb68-7"><a href="statistics-and-machine-learning.html#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lower, <span class="st">&#39;to&#39;</span>, upper)</span>
<span id="cb68-8"><a href="statistics-and-machine-learning.html#cb68-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-9"><a href="statistics-and-machine-learning.html#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify and print rows with outliers</span></span></code></pre></div>
<pre><code>## -824.6236866989851 to 2562.812886698985</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="statistics-and-machine-learning.html#cb70-1" aria-hidden="true" tabindex="-1"></a>outliers <span class="op">=</span> laptops[(laptops[<span class="st">&#39;Price&#39;</span>] <span class="op">&gt;</span> upper) <span class="op">|</span> </span>
<span id="cb70-2"><a href="statistics-and-machine-learning.html#cb70-2" aria-hidden="true" tabindex="-1"></a>                   (laptops[<span class="st">&#39;Price&#39;</span>] <span class="op">&lt;</span> lower)]</span>
<span id="cb70-3"><a href="statistics-and-machine-learning.html#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outliers)</span>
<span id="cb70-4"><a href="statistics-and-machine-learning.html#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="statistics-and-machine-learning.html#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the rows from the dataset</span></span></code></pre></div>
<pre><code>##    Company             Product   Price
## 61    Asus   ROG G703VI-E5062T  3890.0
## 65    Asus  Rog G701VIK-BA060T  2999.0</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="statistics-and-machine-learning.html#cb72-1" aria-hidden="true" tabindex="-1"></a>laptops <span class="op">=</span> laptops[(laptops[<span class="st">&#39;Price&#39;</span>] <span class="op">&lt;=</span> upper) <span class="op">|</span> </span>
<span id="cb72-2"><a href="statistics-and-machine-learning.html#cb72-2" aria-hidden="true" tabindex="-1"></a>                  (laptops[<span class="st">&#39;Price&#39;</span>] <span class="op">&gt;=</span> lower)]</span></code></pre></div>
<ul>
<li>In this scenario, dropping the outliers was likely the right move since the values were unthinkable for laptops prices. This implies that there was some mistake in data entry or data collection. With that being said, this won’t always be the best path forward. It’s important to understand why you got the outliers that you did and if they provide valuable information before you throw them out.</li>
</ul>
<p><strong>Using <code>statsmodels</code> instead.</strong></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="statistics-and-machine-learning.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.proportion <span class="im">import</span> proportion_confint</span>
<span id="cb73-2"><a href="statistics-and-machine-learning.html#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb73-3"><a href="statistics-and-machine-learning.html#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="statistics-and-machine-learning.html#cb73-4" aria-hidden="true" tabindex="-1"></a>heads <span class="op">=</span> binom.rvs(<span class="dv">50</span>, <span class="fl">0.5</span>)</span>
<span id="cb73-5"><a href="statistics-and-machine-learning.html#cb73-5" aria-hidden="true" tabindex="-1"></a>confidence_int <span class="op">=</span> proportion_confint(heads, nobs<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb73-6"><a href="statistics-and-machine-learning.html#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confidence_int)</span>
<span id="cb73-7"><a href="statistics-and-machine-learning.html#cb73-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-8"><a href="statistics-and-machine-learning.html#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat this process 10 times </span></span></code></pre></div>
<pre><code>## (0.37917751674685884, 0.7408224832531413)</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="statistics-and-machine-learning.html#cb75-1" aria-hidden="true" tabindex="-1"></a>heads <span class="op">=</span> binom.rvs(<span class="dv">50</span>, <span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb75-2"><a href="statistics-and-machine-learning.html#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> val <span class="kw">in</span> heads:</span>
<span id="cb75-3"><a href="statistics-and-machine-learning.html#cb75-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for 90% confidence </span></span>
<span id="cb75-4"><a href="statistics-and-machine-learning.html#cb75-4" aria-hidden="true" tabindex="-1"></a>    confidence_interval <span class="op">=</span> proportion_confint(val, <span class="dv">50</span>, <span class="fl">.10</span>)</span>
<span id="cb75-5"><a href="statistics-and-machine-learning.html#cb75-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(confidence_interval)</span></code></pre></div>
<pre><code>## (0.36378436885322046, 0.5962156311467796)
## (0.24834363324703929, 0.4716563667529607)
## (0.42406406993539053, 0.6559359300646095)
## (0.507090652487504, 0.732909347512496)
## (0.3440640699353905, 0.5759359300646095)
## (0.4037843688532205, 0.6362156311467796)
## (0.30518968814451874, 0.5348103118554812)
## (0.30518968814451874, 0.5348103118554812)
## (0.3245317440082245, 0.5554682559917755)
## (0.36378436885322046, 0.5962156311467796)</code></pre>
<ul>
<li>In the loop, there might be at least one confidence interval that does not contain 0.5, the true population proportion for a fair coin flip. You could decrease the likelihood of this happening by increasing your confidence level or lowering the alpha value.</li>
</ul>
<p><strong>Motivating Example:</strong> partial data from an <a href="https://www.kaggle.com/zhangluyuan/ab-testing">AB test</a></p>
<p><em>One-tailed z-test</em></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="statistics-and-machine-learning.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.proportion <span class="im">import</span> proportions_ztest</span>
<span id="cb77-2"><a href="statistics-and-machine-learning.html#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="statistics-and-machine-learning.html#cb77-3" aria-hidden="true" tabindex="-1"></a>conv_rates <span class="op">=</span> results.groupby(<span class="st">&#39;Group&#39;</span>).mean()</span>
<span id="cb77-4"><a href="statistics-and-machine-learning.html#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="co">#control conversions</span></span>
<span id="cb77-5"><a href="statistics-and-machine-learning.html#cb77-5" aria-hidden="true" tabindex="-1"></a>num_control <span class="op">=</span> results[results[<span class="st">&#39;Group&#39;</span>] <span class="op">==</span> <span class="st">&#39;control&#39;</span>][<span class="st">&#39;Converted&#39;</span>].<span class="bu">sum</span>()</span>
<span id="cb77-6"><a href="statistics-and-machine-learning.html#cb77-6" aria-hidden="true" tabindex="-1"></a>total_control <span class="op">=</span> <span class="bu">len</span>(results[results[<span class="st">&#39;Group&#39;</span>] <span class="op">==</span> <span class="st">&#39;control&#39;</span>])</span>
<span id="cb77-7"><a href="statistics-and-machine-learning.html#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="co">#treatment conversions</span></span>
<span id="cb77-8"><a href="statistics-and-machine-learning.html#cb77-8" aria-hidden="true" tabindex="-1"></a>num_treat <span class="op">=</span> results[results[<span class="st">&#39;Group&#39;</span>] <span class="op">==</span> <span class="st">&#39;treatment&#39;</span>][<span class="st">&#39;Converted&#39;</span>].<span class="bu">sum</span>()</span>
<span id="cb77-9"><a href="statistics-and-machine-learning.html#cb77-9" aria-hidden="true" tabindex="-1"></a>total_treat <span class="op">=</span> <span class="bu">len</span>(results[results[<span class="st">&#39;Group&#39;</span>] <span class="op">==</span> <span class="st">&#39;treatment&#39;</span>])</span>
<span id="cb77-10"><a href="statistics-and-machine-learning.html#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="statistics-and-machine-learning.html#cb77-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-12"><a href="statistics-and-machine-learning.html#cb77-12" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> np.array([num_treat, num_control]) </span>
<span id="cb77-13"><a href="statistics-and-machine-learning.html#cb77-13" aria-hidden="true" tabindex="-1"></a>nobs <span class="op">=</span> np.array([total_treat, total_control])</span>
<span id="cb77-14"><a href="statistics-and-machine-learning.html#cb77-14" aria-hidden="true" tabindex="-1"></a>stat, pval <span class="op">=</span> proportions_ztest(count, nobs, alternative<span class="op">=</span><span class="st">&quot;larger&quot;</span>)</span>
<span id="cb77-15"><a href="statistics-and-machine-learning.html#cb77-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="sc">{0:0.3f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(pval))</span></code></pre></div>
<pre><code>## 0.897</code></pre>
<p><em>Two tailed t-test</em></p>
<ul>
<li><em>Data</em> - laptops</li>
</ul>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="statistics-and-machine-learning.html#cb79-1" aria-hidden="true" tabindex="-1"></a>prices <span class="op">=</span> laptops.groupby(<span class="st">&#39;Company&#39;</span>).mean()</span>
<span id="cb79-2"><a href="statistics-and-machine-learning.html#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="statistics-and-machine-learning.html#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign the prices of each group</span></span>
<span id="cb79-4"><a href="statistics-and-machine-learning.html#cb79-4" aria-hidden="true" tabindex="-1"></a>asus <span class="op">=</span> laptops[laptops[<span class="st">&#39;Company&#39;</span>] <span class="op">==</span> <span class="st">&#39;Asus&#39;</span>][<span class="st">&#39;Price&#39;</span>]</span>
<span id="cb79-5"><a href="statistics-and-machine-learning.html#cb79-5" aria-hidden="true" tabindex="-1"></a>toshiba <span class="op">=</span> laptops[laptops[<span class="st">&#39;Company&#39;</span>] <span class="op">==</span> <span class="st">&#39;Toshiba&#39;</span>][<span class="st">&#39;Price&#39;</span>]</span>
<span id="cb79-6"><a href="statistics-and-machine-learning.html#cb79-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-7"><a href="statistics-and-machine-learning.html#cb79-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the t-test</span></span>
<span id="cb79-8"><a href="statistics-and-machine-learning.html#cb79-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb79-9"><a href="statistics-and-machine-learning.html#cb79-9" aria-hidden="true" tabindex="-1"></a>tstat, pval <span class="op">=</span> ttest_ind(asus, toshiba)</span>
<span id="cb79-10"><a href="statistics-and-machine-learning.html#cb79-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="sc">{0:0.3f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(pval))</span></code></pre></div>
<pre><code>## 0.506</code></pre>
</div>
<div id="hypothesis-testing" class="section level4 hasAnchor" number="4.1.3.2">
<h4><span class="header-section-number">4.1.3.2</span> Hypothesis testing<a href="statistics-and-machine-learning.html#hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Hypothesis testing is just a means of coming to some statistical inference. So we want to look at the distribution of the data and come to some conculsion about something that we think may or may not be true.</p>
<p><strong>Assumptions:</strong></p>
<ul>
<li>Random sampling</li>
<li>Independent observations</li>
<li>Normally distributed (large enough sample)</li>
<li>Constant variance</li>
</ul>
<p><em>Null equals no effect, alternative represents that the outcome that the treatement does have a conclusive effect</em></p>
<p><img src="assets/images/two_vs_one_tail_hypotheses.png" width="40%" style="display: block; margin: auto;" /></p>
<p><strong>Which test to use?</strong></p>
<p>The test that is used depends on the situation. If you know the population standard deviation and you have a sufficient sample size - z-test, otherwise t-test.</p>
<p><img src="assets/images/t_test_vs_z_test.png" width="40%" style="display: block; margin: auto;" /></p>
<p><strong>Evaluating results</strong>
When you run your test, your result will be generated in the form of a test statistic, either a z score or t statistic. Using this, you can compute the p-value, which represents the probability of obtaining the sample results you got, given that the null hypothesis is true.</p>
<p><em>Data</em> - <code>results</code> the same as the CI z-test examples above.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="statistics-and-machine-learning.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb81-2"><a href="statistics-and-machine-learning.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb81-3"><a href="statistics-and-machine-learning.html#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="statistics-and-machine-learning.html#cb81-4" aria-hidden="true" tabindex="-1"></a>conv_rates <span class="op">=</span> results.groupby(<span class="st">&#39;Group&#39;</span>).mean()</span>
<span id="cb81-5"><a href="statistics-and-machine-learning.html#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conv_rates)</span></code></pre></div>
<pre><code>##            Converted
## Group               
## control     0.208333
## treatment   0.115385</code></pre>
</div>
<div id="power-and-sample-size" class="section level4 hasAnchor" number="4.1.3.3">
<h4><span class="header-section-number">4.1.3.3</span> Power and sample size<a href="statistics-and-machine-learning.html#power-and-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Components of a power analysis:</p>
<ul>
<li>effect size (e.g. 20% improvement)</li>
<li>significance level / alpha value</li>
<li>power - probability of detecting an effect
<ul>
<li>if we see something then we want to have enough power to conclude with high probability that the results are statistically significant.</li>
<li>lowering power = increasing chance of a Type II error.</li>
</ul></li>
</ul>
<p><img src="assets/images/population_sampling.png" width="40%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="statistics-and-machine-learning.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.power <span class="im">import</span> zt_ind_solve_power</span>
<span id="cb83-2"><a href="statistics-and-machine-learning.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.stats.proportion <span class="im">as</span> prop</span>
<span id="cb83-3"><a href="statistics-and-machine-learning.html#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="statistics-and-machine-learning.html#cb83-4" aria-hidden="true" tabindex="-1"></a>std_effect <span class="op">=</span> prop.proportion_effectsize(<span class="fl">0.20</span>, <span class="fl">0.25</span>)</span>
<span id="cb83-5"><a href="statistics-and-machine-learning.html#cb83-5" aria-hidden="true" tabindex="-1"></a>zt_ind_solve_power(effect_size<span class="op">=</span>std_effect, nobs1<span class="op">=</span><span class="va">None</span>, alpha<span class="op">=</span><span class="fl">0.05</span>, power<span class="op">=</span><span class="fl">0.80</span>)</span>
<span id="cb83-6"><a href="statistics-and-machine-learning.html#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1091.8962 impressions</span></span>
<span id="cb83-7"><a href="statistics-and-machine-learning.html#cb83-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-8"><a href="statistics-and-machine-learning.html#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="co">#increase power to 0.95 -&gt; 800 more observations</span></span>
<span id="cb83-9"><a href="statistics-and-machine-learning.html#cb83-9" aria-hidden="true" tabindex="-1"></a>std_effect <span class="op">=</span> prop.proportion_effectsize(<span class="fl">0.20</span>, <span class="fl">0.25</span>)</span>
<span id="cb83-10"><a href="statistics-and-machine-learning.html#cb83-10" aria-hidden="true" tabindex="-1"></a>zt_ind_solve_power(effect_size<span class="op">=</span>std_effect, nobs1<span class="op">=</span><span class="va">None</span>, alpha<span class="op">=</span><span class="fl">0.05</span>, power<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb83-11"><a href="statistics-and-machine-learning.html#cb83-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1807.76215</span></span></code></pre></div>
<p><strong>Visualizing the relationship between power and sample size</strong></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="statistics-and-machine-learning.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb84-2"><a href="statistics-and-machine-learning.html#cb84-2" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">80</span>)</span>
<span id="cb84-3"><a href="statistics-and-machine-learning.html#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="statistics-and-machine-learning.html#cb84-4" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> np.array(<span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">100</span>))</span>
<span id="cb84-5"><a href="statistics-and-machine-learning.html#cb84-5" aria-hidden="true" tabindex="-1"></a>effect_sizes <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>])</span>
<span id="cb84-6"><a href="statistics-and-machine-learning.html#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.power <span class="im">import</span> TTestIndPower</span>
<span id="cb84-7"><a href="statistics-and-machine-learning.html#cb84-7" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> TTestIndPower()</span>
<span id="cb84-8"><a href="statistics-and-machine-learning.html#cb84-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-9"><a href="statistics-and-machine-learning.html#cb84-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the power analysis</span></span>
<span id="cb84-10"><a href="statistics-and-machine-learning.html#cb84-10" aria-hidden="true" tabindex="-1"></a>results.plot_power(dep_var<span class="op">=</span><span class="st">&#39;nobs&#39;</span>, nobs<span class="op">=</span>sample_sizes, effect_size<span class="op">=</span>effect_sizes)</span>
<span id="cb84-11"><a href="statistics-and-machine-learning.html#cb84-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<ul>
<li>Notice that not only does an increase in power result in a larger sample size, but this increase grows exponentially as the minimum effect size is increased.</li>
</ul>
<p>Tools:</p>
<ul>
<li><code>statsmodel.stats.power</code></li>
<li><code>zt_ind_solve_power()</code></li>
<li><code>tt_ind_solve_power()</code>
<ul>
<li>before we use these methods we need to know <em>standardized minimum effect difference</em> - <code>proportion_effectsize()</code> can do this by inputting baseline and desired minimum conversion rates.</li>
</ul></li>
<li><code>plt_power</code> function in python to visualize</li>
</ul>
</div>
<div id="multiple-testing" class="section level4 hasAnchor" number="4.1.3.4">
<h4><span class="header-section-number">4.1.3.4</span> Multiple testing<a href="statistics-and-machine-learning.html#multiple-testing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When you run a typical hypothesis test with the significance level set to 0.05, there’s a 5 percent chance that you’ll make a type 1 error and detect an effect that doesn’t exist. The multiple comparisons problem arises when you run several sequential hypothesis tests. Since each test is independent, you can multiply the probability of each type I error to get our combined probability of an error. E.g. 20 hypothesis tests of an associations at a 5% significance makes a 65% chance of at least one error.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-56"></span>
<img src="assets/images/multiple_comarison_error.png" alt="Sequential multiple testing increases likelyhood of error." width="30%" />
<p class="caption">
Figure 4.11: Sequential multiple testing increases likelyhood of error.
</p>
</div>
<p><strong>Common approaches to control for mulitiple comparisons</strong></p>
<ol style="list-style-type: decimal">
<li><em>Bonferroni correction</em></li>
<li>Sidak correction</li>
<li>Step-based procedures</li>
<li>Tukey’s procedure</li>
<li>Dunnet’s correction</li>
</ol>
<div id="bonferroni-correction" class="section level5 hasAnchor" number="4.1.3.4.1">
<h5><span class="header-section-number">4.1.3.4.1</span> Bonferroni correction<a href="statistics-and-machine-learning.html#bonferroni-correction" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Bonferroni correction is a common, conservative and an easy approach to perform. You just adjust the p-value as shown below.</p>
<p><img src="assets/images/bonferroni_formula.png" width="30%" style="display: block; margin: auto;" /></p>
<ul>
<li><em>Side effects</em>: Since the approach is a conservative adjustment, with many tests, the corrected significance level will become very, very small. This reduces power, which means that you are increasingly unlikely to detect a true effect when it occurs.</li>
</ul>
<p><em>Without Bonferroni Correction</em></p>
<ul>
<li>Consider a hypothetical situation running 60, 30, and 10 distinct hypothesis tests. Below are the ompute the probability of a Type I error for 60 hypothesis tests with a single-test 5% significance level.</li>
</ul>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="statistics-and-machine-learning.html#cb85-1" aria-hidden="true" tabindex="-1"></a>error_rate_60 <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (<span class="fl">.95</span><span class="op">**</span>(<span class="dv">60</span>))</span>
<span id="cb85-2"><a href="statistics-and-machine-learning.html#cb85-2" aria-hidden="true" tabindex="-1"></a>error_rate_30 <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (<span class="fl">.95</span><span class="op">**</span>(<span class="dv">30</span>))</span>
<span id="cb85-3"><a href="statistics-and-machine-learning.html#cb85-3" aria-hidden="true" tabindex="-1"></a>error_rate_10 <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (<span class="fl">.95</span><span class="op">**</span>(<span class="dv">10</span>))</span>
<span id="cb85-4"><a href="statistics-and-machine-learning.html#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_rate_60)</span></code></pre></div>
<pre><code>## 0.953930201013048</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="statistics-and-machine-learning.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_rate_30)</span></code></pre></div>
<pre><code>## 0.7853612360570628</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="statistics-and-machine-learning.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_rate_10)</span></code></pre></div>
<pre><code>## 0.4012630607616213</code></pre>
<ul>
<li>As you can see, the probability of encountering an error is still extremely high. This is where the Bonferroni correction comes in. While a bit conservative, it controls the family-wise error rate for circumstances like these to avoid the high probability of a Type I error.</li>
</ul>
<p><em>WITH Bonferroni Correction</em></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="statistics-and-machine-learning.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.sandbox.stats.multicomp <span class="im">import</span> multipletests</span>
<span id="cb91-2"><a href="statistics-and-machine-learning.html#cb91-2" aria-hidden="true" tabindex="-1"></a>pvals <span class="op">=</span> [<span class="fl">.01</span>, <span class="fl">.05</span>, <span class="fl">.10</span>, <span class="fl">.50</span>, <span class="fl">.99</span>]</span>
<span id="cb91-3"><a href="statistics-and-machine-learning.html#cb91-3" aria-hidden="true" tabindex="-1"></a>p_adjusted <span class="op">=</span> multipletests(pvals, alpha<span class="op">=</span><span class="fl">0.05</span>, method<span class="op">=</span><span class="st">&#39;bonferroni&#39;</span>)</span>
<span id="cb91-4"><a href="statistics-and-machine-learning.html#cb91-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-5"><a href="statistics-and-machine-learning.html#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the resulting conclusions and adjusted p-values themselves </span></span>
<span id="cb91-6"><a href="statistics-and-machine-learning.html#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_adjusted[<span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [ True False False False False]</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="statistics-and-machine-learning.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_adjusted[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [0.05 0.25 0.5  1.   1.  ]</code></pre>
<ul>
<li>Above the Bonferroni correction, corrected the family-wise error rate for our 5 hypothesis test results. In the end, only one of the tests remained significant.</li>
</ul>
</div>
</div>
</div>
<div id="regression-and-classification" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Regression and Classification<a href="statistics-and-machine-learning.html#regression-and-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="regression-models" class="section level4 hasAnchor" number="4.1.4.1">
<h4><span class="header-section-number">4.1.4.1</span> Regression models<a href="statistics-and-machine-learning.html#regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Regression is a technique used to model and analyze the relationships between variables and how they contribute to producing a particular outcome. In other words, it’s a way to determine which variables have an impact, which factors interact, and how certain we are about those measures.</p>
<p><em>Data</em>: weather data with ,in, max temperature and humidity in morning and afternoon.</p>
<p><strong>Linear and logistic regression:</strong></p>
<div id="linear-regression" class="section level5 hasAnchor" number="4.1.4.1.1">
<h5><span class="header-section-number">4.1.4.1.1</span> 1. Linear regression<a href="statistics-and-machine-learning.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Output is continuous</li>
</ul>
<p><img src="assets/images/linear_regression_formula.png" width="35%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="statistics-and-machine-learning.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co">#what data looks like</span></span>
<span id="cb95-2"><a href="statistics-and-machine-learning.html#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(weather.info())</span></code></pre></div>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
## RangeIndex: 15 entries, 0 to 14
## Data columns (total 4 columns):
## MinTemp        15 non-null float64
## MaxTemp        15 non-null float64
## Humidity9am    15 non-null float64
## Humidity3pm    15 non-null float64
## dtypes: float64(4)
## memory usage: 608.0 bytes
## None</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="statistics-and-machine-learning.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression </span>
<span id="cb97-2"><a href="statistics-and-machine-learning.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb97-3"><a href="statistics-and-machine-learning.html#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb97-4"><a href="statistics-and-machine-learning.html#cb97-4" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">80</span>)</span>
<span id="cb97-5"><a href="statistics-and-machine-learning.html#cb97-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-6"><a href="statistics-and-machine-learning.html#cb97-6" aria-hidden="true" tabindex="-1"></a><span class="co"># reshape(-1,1) needed to ensure has a index value, which indicates to the </span></span>
<span id="cb97-7"><a href="statistics-and-machine-learning.html#cb97-7" aria-hidden="true" tabindex="-1"></a><span class="co">#linear model that the data represent a series of observed values for a single </span></span>
<span id="cb97-8"><a href="statistics-and-machine-learning.html#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="co">#variable. Similar to going wide to long.</span></span>
<span id="cb97-9"><a href="statistics-and-machine-learning.html#cb97-9" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array(weather[<span class="st">&#39;Humidity9am&#39;</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb97-10"><a href="statistics-and-machine-learning.html#cb97-10" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> weather[<span class="st">&#39;Humidity3pm&#39;</span>]</span>
<span id="cb97-11"><a href="statistics-and-machine-learning.html#cb97-11" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb97-12"><a href="statistics-and-machine-learning.html#cb97-12" aria-hidden="true" tabindex="-1"></a>lm.fit(X_train, y_train)</span></code></pre></div>
<pre><code>## LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="statistics-and-machine-learning.html#cb99-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> lm.predict(X_train)</span>
<span id="cb99-2"><a href="statistics-and-machine-learning.html#cb99-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-3"><a href="statistics-and-machine-learning.html#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign and print coefficient </span></span>
<span id="cb99-4"><a href="statistics-and-machine-learning.html#cb99-4" aria-hidden="true" tabindex="-1"></a>coef <span class="op">=</span> lm.coef_</span>
<span id="cb99-5"><a href="statistics-and-machine-learning.html#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(preds)</span></code></pre></div>
<pre><code>## [64.05231955 56.01874907 46.52452942 54.5580999  53.09745072 62.59167037
##  65.51296873 61.86134578 50.17615236 66.24329332 61.13102119 40.68193271
##  71.35556544 75.00718838 31.18771306]</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="statistics-and-machine-learning.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef)</span>
<span id="cb101-2"><a href="statistics-and-machine-learning.html#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="statistics-and-machine-learning.html#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot your fit to visualize your model</span></span></code></pre></div>
<pre><code>## [0.73032459]</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="statistics-and-machine-learning.html#cb103-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train, y_train)</span>
<span id="cb103-2"><a href="statistics-and-machine-learning.html#cb103-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X_train, preds, color<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb103-3"><a href="statistics-and-machine-learning.html#cb103-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="ds-interview-guide_files/figure-html/unnamed-chunk-62-1.png" width="288" /></p>
<ul>
<li>Despite some noise in the plot, we have a decent looking fit here using Humidity9am to predict the dependent variable Humidity3pm with a linear model. Furthermore, take another look at our coefficient. This means that for every 1 unit of humidity in the morning, we can expect about 0.80 units of humidity in the afternoon. More practically, this information tells us that humidity drops about 20% from morning to afternoon!</li>
</ul>
<p><strong>Evaluating models</strong></p>
</div>
<div id="logistic-regression" class="section level5 hasAnchor" number="4.1.4.1.2">
<h5><span class="header-section-number">4.1.4.1.2</span> Logistic regression<a href="statistics-and-machine-learning.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>One of the most common machine learning algorithms for two-class classification</li>
<li>Output is discrete, which allows us to compute probabilities that each observation belongs to a class, thanks to the sigmoid function. The S-shaped curve takes any real number and maps or converts it between 0 and 1.</li>
</ul>
<p><img src="assets/images/logistic_regressions_sigmoid.png" width="45%" style="display: block; margin: auto;" /></p>
<p><img src="assets/images/logistic_regression_formula.png" width="25%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="statistics-and-machine-learning.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb104-2"><a href="statistics-and-machine-learning.html#cb104-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-3"><a href="statistics-and-machine-learning.html#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and fit your model</span></span>
<span id="cb104-4"><a href="statistics-and-machine-learning.html#cb104-4" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression()</span>
<span id="cb104-5"><a href="statistics-and-machine-learning.html#cb104-5" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb104-6"><a href="statistics-and-machine-learning.html#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="statistics-and-machine-learning.html#cb104-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and print the accuracy</span></span></code></pre></div>
<pre><code>## LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
##                    intercept_scaling=1, l1_ratio=None, max_iter=100,
##                    multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;,
##                    random_state=None, solver=&#39;warn&#39;, tol=0.0001, verbose=0,
##                    warm_start=False)
## 
## /Users/BigBrother/anaconda/envs/PyData/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
##   FutureWarning)
## /Users/BigBrother/anaconda/envs/PyData/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
##   y = column_or_1d(y, warn=True)</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="statistics-and-machine-learning.html#cb106-1" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> clf.score(X_test, y_test)</span>
<span id="cb106-2"><a href="statistics-and-machine-learning.html#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc)</span></code></pre></div>
<pre><code>## 0.7708333333333334</code></pre>
<p><strong>Assumptions:</strong></p>
<ol style="list-style-type: decimal">
<li>Linear relationship (check with QQplot)</li>
<li>Errors to be normally distributed ()</li>
<li>Homoscedastic - uniform variance (&lt; 4:1 is usually a good rule of thumb)</li>
<li>Independence</li>
</ol>
</div>
</div>
<div id="evalating-models" class="section level4 hasAnchor" number="4.1.4.2">
<h4><span class="header-section-number">4.1.4.2</span> Evalating models<a href="statistics-and-machine-learning.html#evalating-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Regression:</strong></p>
<ol style="list-style-type: decimal">
<li>R-squared (coefficient of determination) - tells us the proption of variance of the dependent variable that is explained by the regression model.</li>
</ol>
<ul>
<li>usually the first metric checked in regression</li>
<li>can use <code>score</code> in python
<img src="assets/images/r_squared_example.png" width="30%" style="display: block; margin: auto;" /></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><p>MAE (Mean absolute error) - is the sum of the absolute residuals over the number of points</p></li>
<li><p>MSE (Mean squared error) - is the sum of the residuals squared over the number of points.</p></li>
</ol>
<p><img src="assets/images/mse_and_mae.png" width="40%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="statistics-and-machine-learning.html#cb108-1" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> lm.score(X, y)</span>
<span id="cb108-2"><a href="statistics-and-machine-learning.html#cb108-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-3"><a href="statistics-and-machine-learning.html#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, mean_absolute_error</span>
<span id="cb108-4"><a href="statistics-and-machine-learning.html#cb108-4" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> lm.predict(X)</span>
<span id="cb108-5"><a href="statistics-and-machine-learning.html#cb108-5" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y,preds)</span>
<span id="cb108-6"><a href="statistics-and-machine-learning.html#cb108-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-7"><a href="statistics-and-machine-learning.html#cb108-7" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> lm.predict(X)</span>
<span id="cb108-8"><a href="statistics-and-machine-learning.html#cb108-8" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y, preds)</span>
<span id="cb108-9"><a href="statistics-and-machine-learning.html#cb108-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-10"><a href="statistics-and-machine-learning.html#cb108-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(r2)</span></code></pre></div>
<pre><code>## 0.4349683260873261</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="statistics-and-machine-learning.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mse)</span></code></pre></div>
<pre><code>## 230.88845883712395</code></pre>
<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="statistics-and-machine-learning.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mae)</span></code></pre></div>
<pre><code>## 11.818938818487227</code></pre>
<ul>
<li>Note that our R-squared value tells us the percentage of the variance of y that X is responsible for.</li>
</ul>
<p><strong>Classification</strong></p>
<ol style="list-style-type: decimal">
<li>Precision - can be interpreted as the percentage of observations that you correctly guessed and is linked to the rate of type 1 error.</li>
</ol>
<p><img src="assets/images/classification_precision.png" width="30%" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Recall - linked to the rate of type II errors.</li>
</ol>
<p><img src="assets/images/classification_recall.png" width="30%" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li>Confusion matrices - shown earlier, but you can use them to determine which error type you would like to prioritize.</li>
</ol>
<ul>
<li>E.g. A spam detector would probably mean that you don’t want to make any type 1 errors (i.e. don’t want to miss real emails that are marked as spam) and want to optimize for precision.</li>
<li>E.g. Classifying a rare disease you want to avoid type II errors (i.e. ), and you would want to optimize for recall.</li>
</ul>
<div class="sourceCode" id="cb114"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="statistics-and-machine-learning.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, precision_score, recall_score</span>
<span id="cb114-2"><a href="statistics-and-machine-learning.html#cb114-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb114-3"><a href="statistics-and-machine-learning.html#cb114-3" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> confusion_matrix(y_test, preds)</span>
<span id="cb114-4"><a href="statistics-and-machine-learning.html#cb114-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(matrix)</span></code></pre></div>
<pre><code>## [[185   0]
##  [ 55   0]]</code></pre>
<div class="sourceCode" id="cb116"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="statistics-and-machine-learning.html#cb116-1" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_test, preds)</span>
<span id="cb116-2"><a href="statistics-and-machine-learning.html#cb116-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Using precision since you don&#39;t want to say it will rain when it won&#39;t</span></span></code></pre></div>
<pre><code>## /Users/BigBrother/anaconda/envs/PyData/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
##   &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="statistics-and-machine-learning.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(precision)</span></code></pre></div>
<pre><code>## 0.0</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="statistics-and-machine-learning.html#cb120-1" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_test, preds)</span>
<span id="cb120-2"><a href="statistics-and-machine-learning.html#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(recall)</span></code></pre></div>
<pre><code>## 0.0</code></pre>
<ul>
<li>You can see here that the precision of our rain prediction model was quite high, meaning that we didn’t make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on.</li>
</ul>
</div>
<div id="missing-data-and-outliers" class="section level4 hasAnchor" number="4.1.4.3">
<h4><span class="header-section-number">4.1.4.3</span> Missing data and outliers<a href="statistics-and-machine-learning.html#missing-data-and-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>How to handle missing data:</em></p>
<ul>
<li><p>Drop the row - easy but could lose data from your dataset that could strengthen the model</p>
<ul>
<li><code>df.dropna(inplace=True)</code></li>
</ul></li>
<li><p>Impute the missing values</p>
<ol style="list-style-type: decimal">
<li>constant value</li>
<li>randomly select record value</li>
<li>mean, median, mode</li>
<li>value estimated by another model (multiple imputation)</li>
</ol></li>
</ul>
<p><em>Dealing with Outliers</em></p>
<ul>
<li>Standard Deviations - any observations that falls outside of 3 standard deviations from the mean is deemed an outlier.</li>
<li>Interquartile range (IQR) - Q1 and Q3 minus (1.5 * IQR). IQR is the difference between Q1 and Q3</li>
</ul>
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="statistics-and-machine-learning.html#cb122-1" aria-hidden="true" tabindex="-1"></a>nulls <span class="op">=</span> laptops[laptops.isnull().<span class="bu">any</span>(axis<span class="op">=</span><span class="dv">1</span>)]</span>
<span id="cb122-2"><a href="statistics-and-machine-learning.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(nulls)</span></code></pre></div>
<pre><code>## Empty DataFrame
## Columns: [Company, Product, Price]
## Index: []</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="statistics-and-machine-learning.html#cb124-1" aria-hidden="true" tabindex="-1"></a>laptops.fillna(<span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb124-2"><a href="statistics-and-machine-learning.html#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(laptops.head())</span></code></pre></div>
<pre><code>##   Company                                    Product   Price
## 0    Acer                                   Aspire 3  400.00
## 1    Asus                            Vivobook E200HA  191.90
## 2    Asus       E402WA-GA010T (E2-6110/2GB/32GB/W10)  199.00
## 3    Asus  X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux)  389.00
## 4    Asus     X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce  522.99</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="statistics-and-machine-learning.html#cb126-1" aria-hidden="true" tabindex="-1"></a>laptops.fillna(laptops.Price.median(), inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb126-2"><a href="statistics-and-machine-learning.html#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(laptops.head())</span></code></pre></div>
<pre><code>##   Company                                    Product   Price
## 0    Acer                                   Aspire 3  400.00
## 1    Asus                            Vivobook E200HA  191.90
## 2    Asus       E402WA-GA010T (E2-6110/2GB/32GB/W10)  199.00
## 3    Asus  X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux)  389.00
## 4    Asus     X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce  522.99</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="statistics-and-machine-learning.html#cb128-1" aria-hidden="true" tabindex="-1"></a>laptops.dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb128-2"><a href="statistics-and-machine-learning.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(laptops.head())</span></code></pre></div>
<pre><code>##   Company                                    Product   Price
## 0    Acer                                   Aspire 3  400.00
## 1    Asus                            Vivobook E200HA  191.90
## 2    Asus       E402WA-GA010T (E2-6110/2GB/32GB/W10)  199.00
## 3    Asus  X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux)  389.00
## 4    Asus     X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce  522.99</code></pre>
</div>
<div id="unbalanced-data" class="section level4 hasAnchor" number="4.1.4.4">
<h4><span class="header-section-number">4.1.4.4</span> Unbalanced data<a href="statistics-and-machine-learning.html#unbalanced-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/">5 techniques to handle unbalanced data</a></li>
<li><a href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/">SMOTE</a></li>
</ul>
</div>
<div id="bias-variance-tradeoff" class="section level4 hasAnchor" number="4.1.4.5">
<h4><span class="header-section-number">4.1.4.5</span> Bias-variance tradeoff<a href="statistics-and-machine-learning.html#bias-variance-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Types of error in modeling:</em></p>
<ol style="list-style-type: decimal">
<li>Bias error - comes from simplifying assumptions made by a model to make the target function easier to learn. In general, high bias makes algorithms faster to learn and easier to understand but less flexible. Too much bias can lead to a problem with <code>under-fitting</code> the data, which happens when the model is making too many assumptions. High biased algorithms include: linear regression, LDA, and logisitic regression.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-76"></span>
<img src="assets/images/biased_error_example.png" alt="Line is high biased since it is going in a straight line when the data isn't linear." width="40%" />
<p class="caption">
Figure 4.12: Line is high biased since it is going in a straight line when the data isn’t linear.
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Variance error - variance is the amount that the estimate of the target function would change if different training data was used. Some variance will exist but, ideally the results wouldn’t change too much from one training dataset to the next. Too much variance in the model leads to <code>overfitting</code>. It happens when the model is too flexible and fits too closely to the training data, making it not generalizable to unseen data. High variance algorithms include: Decision Trees, k-Nearest Neighbors, and SVM.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-77"></span>
<img src="assets/images/variance_error_example.png" alt="Line is high variance since it too closely follows the data." width="40%" />
<p class="caption">
Figure 4.13: Line is high variance since it too closely follows the data.
</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Irreducible error - won’t focus on.</li>
</ol>
<p><em>Goal of modeling</em></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-78"></span>
<img src="assets/images/bias_variance_goal.png" alt="Want to balance model complexity to minimize both bias and variance." width="40%" />
<p class="caption">
Figure 4.14: Want to balance model complexity to minimize both bias and variance.
</p>
</div>
</div>
</div>
</div>
<div id="sample-questions-2" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Sample Questions<a href="statistics-and-machine-learning.html#sample-questions-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="comparison-of-means" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Comparison of means<a href="statistics-and-machine-learning.html#comparison-of-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So suppose hypothetically Facebook users share, their height in centimeters and their gender. How would you test the hypothesis that men on average are taller?</p>
<p>t-test could be used to compare the means of two different groups if the sample is larger then 30, but if larger then 30 we can use a z-test.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="statistics-and-machine-learning.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.weightstats <span class="im">import</span> ztest <span class="im">as</span> ztest</span>
<span id="cb130-2"><a href="statistics-and-machine-learning.html#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="statistics-and-machine-learning.html#cb130-3" aria-hidden="true" tabindex="-1"></a>women <span class="op">=</span> [<span class="dv">82</span>, <span class="dv">84</span>, <span class="dv">85</span>, <span class="dv">89</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">92</span>, <span class="dv">94</span>, <span class="dv">99</span>, <span class="dv">99</span>,</span>
<span id="cb130-4"><a href="statistics-and-machine-learning.html#cb130-4" aria-hidden="true" tabindex="-1"></a>         <span class="dv">82</span>, <span class="dv">84</span>, <span class="dv">85</span>, <span class="dv">89</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">92</span>, <span class="dv">94</span>, <span class="dv">99</span>, <span class="dv">99</span>,</span>
<span id="cb130-5"><a href="statistics-and-machine-learning.html#cb130-5" aria-hidden="true" tabindex="-1"></a>         <span class="dv">105</span>, <span class="dv">109</span>, <span class="dv">109</span>, <span class="dv">109</span>, <span class="dv">110</span>, <span class="dv">112</span>, <span class="dv">112</span>, <span class="dv">113</span>, <span class="dv">114</span>, <span class="dv">114</span>,</span>
<span id="cb130-6"><a href="statistics-and-machine-learning.html#cb130-6" aria-hidden="true" tabindex="-1"></a>         <span class="dv">105</span>, <span class="dv">109</span>, <span class="dv">109</span>, <span class="dv">109</span>, <span class="dv">110</span>, <span class="dv">112</span>, <span class="dv">112</span>, <span class="dv">113</span>, <span class="dv">114</span>, <span class="dv">114</span>]</span>
<span id="cb130-7"><a href="statistics-and-machine-learning.html#cb130-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-8"><a href="statistics-and-machine-learning.html#cb130-8" aria-hidden="true" tabindex="-1"></a>men <span class="op">=</span> [<span class="dv">90</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">95</span>, <span class="dv">95</span>, <span class="dv">99</span>, <span class="dv">99</span>, <span class="dv">108</span>, <span class="dv">109</span>,</span>
<span id="cb130-9"><a href="statistics-and-machine-learning.html#cb130-9" aria-hidden="true" tabindex="-1"></a>       <span class="dv">90</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">95</span>, <span class="dv">95</span>, <span class="dv">99</span>, <span class="dv">99</span>, <span class="dv">108</span>, <span class="dv">109</span>,</span>
<span id="cb130-10"><a href="statistics-and-machine-learning.html#cb130-10" aria-hidden="true" tabindex="-1"></a>       <span class="dv">109</span>, <span class="dv">114</span>, <span class="dv">115</span>, <span class="dv">116</span>, <span class="dv">117</span>, <span class="dv">117</span>, <span class="dv">128</span>, <span class="dv">129</span>, <span class="dv">130</span>, <span class="dv">133</span>,</span>
<span id="cb130-11"><a href="statistics-and-machine-learning.html#cb130-11" aria-hidden="true" tabindex="-1"></a>       <span class="dv">109</span>, <span class="dv">114</span>, <span class="dv">115</span>, <span class="dv">116</span>, <span class="dv">117</span>, <span class="dv">117</span>, <span class="dv">128</span>, <span class="dv">129</span>, <span class="dv">130</span>, <span class="dv">133</span>]</span>
<span id="cb130-12"><a href="statistics-and-machine-learning.html#cb130-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-13"><a href="statistics-and-machine-learning.html#cb130-13" aria-hidden="true" tabindex="-1"></a><span class="co">#perform two sample z-test</span></span>
<span id="cb130-14"><a href="statistics-and-machine-learning.html#cb130-14" aria-hidden="true" tabindex="-1"></a>ztest(women, men, value<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb130-15"><a href="statistics-and-machine-learning.html#cb130-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-16"><a href="statistics-and-machine-learning.html#cb130-16" aria-hidden="true" tabindex="-1"></a><span class="co">#(-1.9953236073282115, 0.046007596761332065)</span></span></code></pre></div>
<pre><code>## (-2.8587017261290355, 0.004253785496952403)</code></pre>
<ul>
<li>The test statistic for the two sample z-test is -1.9953 and the corresponding p-value is 0.0460. Since this p-value is less than .05, we have sufficient evidence to reject the null hypothesis. In other words, the mean height of men is significantly different vs women.</li>
</ul>
<div class="sourceCode" id="cb132"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="statistics-and-machine-learning.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb132-2"><a href="statistics-and-machine-learning.html#cb132-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-3"><a href="statistics-and-machine-learning.html#cb132-3" aria-hidden="true" tabindex="-1"></a>women <span class="op">=</span> [<span class="dv">82</span>, <span class="dv">84</span>, <span class="dv">85</span>, <span class="dv">89</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">92</span>, <span class="dv">94</span>, <span class="dv">99</span>, <span class="dv">99</span>,</span>
<span id="cb132-4"><a href="statistics-and-machine-learning.html#cb132-4" aria-hidden="true" tabindex="-1"></a>         <span class="dv">105</span>, <span class="dv">109</span>, <span class="dv">109</span>, <span class="dv">109</span>, <span class="dv">110</span>, <span class="dv">112</span>, <span class="dv">112</span>, <span class="dv">113</span>, <span class="dv">114</span>, <span class="dv">114</span>]</span>
<span id="cb132-5"><a href="statistics-and-machine-learning.html#cb132-5" aria-hidden="true" tabindex="-1"></a>men <span class="op">=</span> [<span class="dv">90</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">91</span>, <span class="dv">95</span>, <span class="dv">95</span>, <span class="dv">99</span>, <span class="dv">99</span>, <span class="dv">108</span>, <span class="dv">109</span>,</span>
<span id="cb132-6"><a href="statistics-and-machine-learning.html#cb132-6" aria-hidden="true" tabindex="-1"></a>       <span class="dv">109</span>, <span class="dv">114</span>, <span class="dv">115</span>, <span class="dv">116</span>, <span class="dv">117</span>, <span class="dv">117</span>, <span class="dv">128</span>, <span class="dv">129</span>, <span class="dv">130</span>, <span class="dv">133</span>]</span>
<span id="cb132-7"><a href="statistics-and-machine-learning.html#cb132-7" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb132-8"><a href="statistics-and-machine-learning.html#cb132-8" aria-hidden="true" tabindex="-1"></a><span class="co">#check for equal variances, we can assume the populations have equal variances </span></span>
<span id="cb132-9"><a href="statistics-and-machine-learning.html#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="co">#if the ratio of the larger sample variance to the smaller sample variance </span></span>
<span id="cb132-10"><a href="statistics-and-machine-learning.html#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="co">#is less than 4:1. </span></span>
<span id="cb132-11"><a href="statistics-and-machine-learning.html#cb132-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.var(women), np.var(men))</span></code></pre></div>
<pre><code>## 119.92750000000001 197.06</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="statistics-and-machine-learning.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb134-2"><a href="statistics-and-machine-learning.html#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="statistics-and-machine-learning.html#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="co">#perform two sample t-test with equal variances</span></span>
<span id="cb134-4"><a href="statistics-and-machine-learning.html#cb134-4" aria-hidden="true" tabindex="-1"></a>stats.ttest_ind(a<span class="op">=</span>women, b<span class="op">=</span>men, equal_var<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<pre><code>## Ttest_indResult(statistic=-1.9953236073282115, pvalue=0.05321388037191098)</code></pre>
<ul>
<li>Because the p-value of our test (0.5321) is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height of men vs women is different.</li>
</ul>
</div>
<div id="principle-of-inclusionexclusion" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Principle of Inclusion/Exclusion<a href="statistics-and-machine-learning.html#principle-of-inclusionexclusion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The carshare dilemma
* Statistics
* Probability Theory</p>
<p>Suppose we have selected a group of people to take a survey. 35% of the group like Uber, 20% like both Lyft and Uber, and 25% like neither Lyft nor Uber.</p>
<p>Given this information, what percentage of the sample likes Lyft?</p>
<p>Hint: You can use basic probability theory to solve this problem.</p>
<p><a href="https://lah.elearningontario.ca/CMS/public/exported_courses/MDM4U/exported/MDM4UU01/MDM4UU01/MDM4UU01A03/_content.html">Tip 1:</a></p>
<p><a href="https://www.mbacrystalball.com/blog/2015/10/09/set-theory-tutorial/">Tip 2:</a></p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-82"></span>
<img src="assets/images/prob_theory_1.jpeg" alt="carshare dilema" width="100%" />
<p class="caption">
Figure 4.15: carshare dilema
</p>
</div>
</div>
<div id="bayes-theorem-1" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Bayes Theorem 1<a href="statistics-and-machine-learning.html#bayes-theorem-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Probability of passing through interview stages</p>
<ul>
<li>Statistics</li>
<li>Bayes Theorem</li>
</ul>
<p>Given the information below, if you had a good first interview, what is the probability you will receive a second interview?</p>
<ol style="list-style-type: decimal">
<li>50% of all people who received a first interview received a second interview</li>
<li>95% of people that received a second interview had a good first interview</li>
<li>75% of people that did not receive a second interview had a good first interview</li>
</ol>
<p><a href="https://stats.stackexchange.com/questions/86015/amazon-interview-question-probability-of-2nd-interview">Solution borrowed from here</a></p>
<p><a href="https://www.statisticshowto.com/probability-and-statistics/probability-main-index/bayes-theorem-problems/#:~:text=Bayes&#39;%20Theorem%20Example%20%231&amp;text=A%20could%20mean%20the%20event,P(B)%20%3D%200.05.">Other good source on Bayes Theorem</a></p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-83"></span>
<img src="assets/images/bayes_1.jpeg" alt="Interview Bayes Theorem" width="100%" />
<p class="caption">
Figure 4.16: Interview Bayes Theorem
</p>
</div>
</div>
<div id="mae-vs-mse" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> MAE vs MSE<a href="statistics-and-machine-learning.html#mae-vs-mse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Statistics</li>
<li>Model Evaluation</li>
</ul>
<p>What are some of the differences you would expect in a model that minimizes squared error, vs a model that minimizes absolute error? In which cases would each error metric be appropriate?</p>
<p><em>Typically, if the dataset has outliers or you’re worried about individual observations, you’ll want to use MSE, since by squaring the errors, they are weighted more heavily. If you aren’t concerned with outliers or single observations then MAE can be used to suppress those errors a bit more, since the absolute values is taken and not the square.</em></p>
</div>
<div id="bias-variance-tradeoff-1" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Bias variance tradeoff<a href="statistics-and-machine-learning.html#bias-variance-tradeoff-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Statisitics</li>
<li>Data modeing</li>
</ul>
<p>In terms of the bias-variance tradeoff, which of the following is substantially more harmful to the test error than the training error?</p>
<p><em>High variance results in overfitting to your training set. You’ll see strong performance at first, until you apply your model to your test set, where it will fail to generalize and likely struggle.</em></p>
</div>
<div id="changing-the-companys-color" class="section level3 hasAnchor" number="4.2.6">
<h3><span class="header-section-number">4.2.6</span> Changing the company’s color<a href="statistics-and-machine-learning.html#changing-the-companys-color" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Statistics</li>
<li>Hypothesis Testing</li>
</ul>
<p>Suppose you work for an eCommerce company that sells cookies. The design team changed the company’s logo color from blue to red, which caused many downstream changes to the company’s landing page, ad creatives, and product packaging. The team did not did not perform an A/B test prior to the change. The lead designer wants to understand if the new color has impacted sales. Explain how you would design an analysis to understand sales impact.</p>
</div>
<div id="repeated-significance-testing-error" class="section level3 hasAnchor" number="4.2.7">
<h3><span class="header-section-number">4.2.7</span> Repeated significance testing error<a href="statistics-and-machine-learning.html#repeated-significance-testing-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Statistics</li>
<li>Communication</li>
</ul>
<p>Context: Your colleagues will often come to you with methodology questions. Please answer the following questions as concisely as possible. Imagine you are responding via email and the person receiving your email is non-technical.</p>
<p>1.1 Your colleague started an A/B test 7 days ago, and they have been checking the p-value every day using an A/B test calculator. Today it finally dipped to 0.04, and they are very excited to move forward with the winner.</p>
<p>What recommendations would you make to your non-technical colleague, considering the reported false positive rate? Why?</p>
<p><strong>The colleague made a mistake by not following the parameters set at the start of the A/B test. The significance calculation (p-value here) makes a critical assumption that you have probably violated without even realizing it: that the sample size was fixed in advance. If instead of deciding ahead of time, “this experiment will collect exactly 1,000 observations,” you say, “we’ll run it until we see a significant difference,” all the reported significance levels become meaningless.</strong></p>
<p><strong>A response could include the fact that a day later the p-value could again dip above the significance test, but because we used a power calculation before hand we need to stick with our original parameters or we risk getting a false positive and could lead to a unexpected decrease in what the test was originally designed to improve.</strong></p>
<p><a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">Reference</a></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="python-r-analyses.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="case-studies.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/elreb/data_science_prep/master/04-stats-ml.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ds-interview-guide.pdf", "ds-interview-guide.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
