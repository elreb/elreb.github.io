[["index.html", "Data Science Prep Guide Welcome to the Data Science Prep Guide", " Data Science Prep Guide Updated: 2022-06-13 Welcome to the Data Science Prep Guide This book will go over various questions in preparation for Data Scientist interviews. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Data Scientist interviews are tough! Depending on the role that you are interviewing for a interview panel may focus on your technical skills, your business acumen, communications skills and overall fit for the company. This prep guide, is broken up into various sections of a potential interview. 1.0.1 Sections SQL technical questions Python / R analyses Statistics/Machine Learning Case Study Behavioral Questions Computer Science / Data Structures and Algorithms questions "],["sql-questions.html", "Chapter 2 SQL questions 2.1 SQL tips 2.2 Sample Questions", " Chapter 2 SQL questions 2.1 SQL tips Some keywords are especially useful for SQL interview problems. Here are some that I suggest to know how to use well: INNER JOIN, LEFT JOIN, FULL JOIN - just know them! COALESCE textbook definition - function returns the first non-NULL expression in the specified list. If all the arguments are NULL then it will return NULL as its output. use to replace certain values to a default value (but in interviews mostly use for NULL or outliers). CASE WHEN - expression is the same as IF/ELSE statement in other programming languages. e.g.  SELECT title, rating, CASE rating WHEN &#39;G&#39; THEN &#39;General Audiences&#39; WHEN &#39;PG&#39; THEN &#39;Parental Guidance Suggested&#39; WHEN &#39;PG-13&#39; THEN &#39;Parents Strongly Cautioned&#39; WHEN &#39;R&#39; THEN &#39;Restricted&#39; WHEN &#39;NC-17&#39; THEN &#39;Adults Only&#39; END rating_description FROM film ORDER BY title; SUM CASE WHEN - can use to aggregate a count of a certain row category. e.g. SELECT SUM(CASE rating WHEN &#39;G&#39; THEN 1 ELSE 0 END) &quot;General Audiences&quot; FROM film ROUND - when using floats or making percentages just use it for more readable output. Common Table Expressions (CTEs), and subqueries - know how to use them, you’ll need them! HAVING vs WHERE - The difference between the having and where clause in SQL is that the where clause cannot be used with aggregates, but the having clause can. The where clause works on row’s data, not on aggregated data. Casting data types with CAST or :: Misc window functions - which perform an operation across a set of rows that are somehow related to the current row. Fetching - Relative LAG(column, n) returns column’s value at the row n rows before the current row LEAD(column, n) returns column’s value at the row n rows after the current row {r echo=FALSE, out.width = '65 %', fig.align = 'center'} knitr::include_graphics(\"assets/images/sql_lead_ex.png\") Fetching - Absolute FIRST_VALUE(column) returns the first value in the table or partition LAST_VALUE(column) returns the last value in the table or partition Figure 2.1: RANGE BETWEEN clause extends the window to the end of the table or partition. ROW_NUMBER() always assigns unique numbers, even if two rows’ values are the same RANK() assigns the same number to rows with identical values, skipping over the next numbers in such cases DENSE_RANK() also assigns the same number to rows with identical values, but doesn’t skip over the next numbers Frames ROWS BETWEEN [START] AND [FINISH] n PRECEDING: n rows before the current row CURRENT ROW: the current row n FOLLOWING: n rows after the current row 2.2 Sample Questions 2.2.1 Calculate the 30 day readmission rate Table: encounter Columns: patient_id, encounter_id, encounter_start, encounter_end, encounter_class (in patient) sql fiddle CREATE TABLE encounter (patient_id int, admid int, adm_date date, dis_date date, encounter_class int) ; INSERT INTO encounter (patient_id, admid, adm_date, dis_date, encounter_class) VALUES (0001, 10000, &#39;2017-01-01 04:05:06&#39;, &#39;2017-01-05 04:05:06&#39;, 5), (0002, 10001, &#39;2017-02-01 04:05:06&#39;, &#39;2017-02-02 04:05:06&#39;, 3), (0003, 10002, &#39;2017-03-01 04:05:06&#39;, &#39;2017-03-04 04:05:06&#39;, 2), (0004, 10003, &#39;2017-04-01 04:05:06&#39;, &#39;2017-04-15 04:05:06&#39;, 4), (0001, 10004, &#39;2017-05-01 04:05:06&#39;, &#39;2017-05-02 04:05:06&#39;, 2), (0005, 10005, &#39;2017-06-01 04:05:06&#39;, &#39;2017-06-05 04:05:06&#39;, 1), (0001, 10006, &#39;2017-04-01 04:05:06&#39;, &#39;2017-04-03 04:05:06&#39;, 1), (0002, 10007, &#39;2017-03-01 04:05:06&#39;, &#39;2017-03-15 04:05:06&#39;, 7), (0006, 10008, &#39;2017-02-01 04:05:06&#39;, &#39;2017-02-05 04:05:06&#39;, 2), (0003, 10009, &#39;2017-07-01 04:05:06&#39;, &#39;2017-07-03 04:05:06&#39;, 1), (0007, 10010, &#39;2017-09-01 04:05:06&#39;, &#39;2017-09-20 04:05:06&#39;, 2) ; /* make indicator for patient id, if readmitted within 30 days merge cte for total count of unique admissions*/ WITH next_date_tbl AS ( SELECT patient_id, adm_date, LEAD(adm_date, 1) OVER ( PARTITION BY patient_id ORDER BY adm_date ) AS next_date FROM encounter ), date_dif_tbl AS ( SELECT patient_id, (CAST(next_date AS DATE) - CAST(adm_date AS DATE)) AS date_dif FROM next_date_tbl ), num_denom AS ( SELECT SUM(CASE WHEN date_dif &lt;= 30 THEN 1 ELSE 0 END) AS readmitted, COUNT(*) AS total_cnt FROM date_dif_tbl ) SELECT (ROUND((readmitted::float/total_cnt::float)::numeric,2) * 100) AS thirty_day_readmission_rt FROM num_denom 2.2.2 Top 10 diagnoses from encounters with a length of service greater then 3 days SQL Database Querying Health records Table: encounter Columns: encounter_id, diagnosis_id, priority, encounter_start, encounter_end sql fiddle CREATE TABLE encounters (encounter_id int PRIMARY KEY, diagnosis_id int, encounter_start TIMESTAMP, encounter_end TIMESTAMP); INSERT INTO encounters (encounter_id, diagnosis_id, encounter_start, encounter_end) VALUES (1, 5, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (2, 5, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (3, 5, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (4, 2, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (5, 2, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (6, 3, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-09 04:05:06&#39;), (7, 4, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-09 04:05:06&#39;), (8, 4, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-09 04:05:06&#39;), (9, 1, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;) ; WITH date_calc AS ( SELECT *, (CAST(encounter_end AS date) - CAST(encounter_start AS date)) AS los FROM encounters ), grouped_diagnosis AS ( SELECT diagnosis_id, COUNT(*) AS n FROM date_calc WHERE los &gt; 3 GROUP BY diagnosis_id ) SELECT diagnosis_id FROM grouped_diagnosis ORDER BY n DESC LIMIT 10; 2.2.3 Employee survey results Suppose you’re consulting for a company, and you’ve sent out a survey that asks successive qustions randomly. The survey logs data into a table called survey_logging. The schema of the table is: survey_logging Column Name Data Type Description employee_id integer employee id of the survey respondant action string Will be one of the following values, ‘view,’ ‘answer,’ ‘skip’ question_id integer ID of the question asked answer_id integer ID of the answer asked timestamp integer time stamp of the action made by respondant Using SQL, find which question has the highest response rate. db_fiddle /*Intermediate result would look like: | question_id | response_rate | */ CREATE TABLE survey_logging (user_id int PRIMARY KEY, &quot;action&quot; varchar(10), question_id int) ; INSERT INTO survey_logging (user_id, &quot;action&quot;, question_id) VALUES (1, &#39;answer&#39;, 1), (2, &#39;answer&#39;, 1), (3,&#39;answer&#39;, 2), (4,&#39;skip&#39;, 2), (5,&#39;skip&#39;, 1), (6,&#39;answer&#39;, 1), (7,&#39;answer&#39;, 1), (8,&#39;answer&#39;, 1), (9,&#39;skip&#39;, 1), (10,&#39;skip&#39;, 2) ; /*~~~~~~~~~~~~~~~~~~~~~*/ WITH response_by_q AS ( SELECT question_id, SUM (CASE WHEN action = &#39;answer&#39; THEN 1 ELSE 0 END) AS answered, COUNT(*) AS ovr_count FROM survey_logging GROUP BY question_id ), extra_column AS ( SELECT question_id, ROUND(answered * 100.0 / ovr_count, 1) AS answer FROM response_by_q ) SELECT question_id FROM extra_column WHERE answer = (SELECT MAX(answer) FROM extra_column) 2.2.4 Calculating student attendance Given the following table schemas, how would you figure out the overall attendance rate for each grade on 2022-01-08? Solution will be written in SQL for premium users. Table 1: student_attendance_log Column Name Data Type Description date string date of log per student_id, format is timestamp student_id integer id of the student attendance_status string Possible values are [‘present,’ ‘tardy,’ ‘absent’] Table 2: student_demographic Column Name Data Type Description student_id integer id of the student grade_level integer will be a value between 0-12, which corresponds date_of_birth string Student birth date, format is ‘yyyy-mm-dd’ db_fiddle CREATE TABLE student_attendence_log (student_id int PRIMARY KEY, &quot;date&quot; varchar, attendence_status varchar); INSERT INTO student_attendence_log (student_id, &quot;date&quot;, attendence_status) VALUES (1, &#39;2022-01-08 04:05:06&#39;, &#39;present&#39;), (2, &#39;2022-01-08 04:05:06&#39;, &#39;present&#39;), (3, &#39;2022-01-08 04:05:06&#39;, &#39;tardy&#39;), (4, &#39;2022-01-08 04:05:06&#39;, &#39;present&#39;), (5, &#39;2022-01-08 04:05:06&#39;, &#39;absent&#39;), (6, &#39;2022-01-08 04:05:06&#39;, &#39;present&#39;), (7, &#39;2022-01-08 04:05:06&#39;, &#39;tardy&#39;), (8, &#39;2022-01-08 04:05:06&#39;, &#39;present&#39;), (9, &#39;2022-01-08 04:05:06&#39;, &#39;tardy&#39;) ; CREATE TABLE student_demographic (student_id int PRIMARY KEY, grade_level varchar); INSERT INTO student_demographic (student_id, grade_level) VALUES (1, &#39;11&#39;), (2, &#39;12&#39;), (3, &#39;12&#39;), (4, &#39;10&#39;), (5, &#39;11&#39;), (6, &#39;11&#39;), (7, &#39;12&#39;), (8, &#39;10&#39;), (9, &#39;10&#39;) ; -- table 1 date and attendence &#39;present&#39; and total -- aggregate on grade table 2 WITH merged AS ( SELECT &quot;date&quot;::date, a.student_id, a.attendence_status, b.grade_level FROM student_demographic b LEFT JOIN student_attendence_log a ON a.student_id = b.student_id WHERE &quot;date&quot;::date = to_date(&#39;2022-01-08&#39;,&#39;YYYY-MM-DD&#39;) ), present_vs_total AS ( SELECT SUM(CASE WHEN attendence_status = &#39;present&#39; THEN 1 ELSE 0 END) AS present, COUNT(*) AS total, grade_level FROM merged GROUP BY grade_level ) SELECT grade_level, ROUND((present::float / total::float)::numeric,2) AS attendence_rate FROM present_vs_total 2.2.5 Music streaming behavior Below are two table schemas for a popular music streaming application: Table 1: user_song_log Column Name Data Type Description user_id id id of the streaming user timestamp integer timestamp of when the user started listening to the song, epoch seconds song_id integer id of the song artist_id integer id of the artist Table 2: song_info Column Name Data Type Description song_id integer id of the song artist_id integer id of the artist song_length integer length of song in seconds Question: Given the above, can you write a SQL query to estimate the average number of hours a user spends listening to music daily? You can assume that once a given user starts a song, they listen to it in its entirety. dbfiddle CREATE TABLE user_song_log (user_id int, &quot;timestamp&quot; bigint, song_id int); INSERT INTO user_song_log (user_id, &quot;timestamp&quot;, song_id) VALUES (1, 1655081554, 1), (2, 1615080554, 1), (3, 1635071554, 2), (4, 1655081544, 3), (2, 1655081584, 2), (1, 1635081554, 4), (3, 1655081532, 3), (3, 1625081524, 3), (2, 1645081551, 2) ; CREATE TABLE song_info (song_id int, song_length int); INSERT INTO song_info (song_id, song_length) VALUES (1, 120), (2, 200), (3, 134), (4, 233) ; -- hours is 60 * 60 from seconds WITH song_length_by_day AS ( SELECT SUM(song_length) AS song_length_by_day FROM ( SELECT a.user_id, to_timestamp(a.&quot;timestamp&quot;)::date AS day, a.song_id, b.song_length FROM user_song_log a INNER JOIN song_info b ON a.song_id = b.song_id) joined GROUP BY user, day ) SELECT AVG((song_length_by_day::float / 60)/60) AS &quot;Average hours spent listening to music daily&quot; FROM song_length_by_day 2.2.6 A hotel chain’s loyal customers Suppose you’re an analyst for a major US hotel chain which has locations all over the US. Your marketing team is planning a promotion focused around loyal customers, and they are trying to forecast how much revenue the promotion will bring in. However, they need help from you to understand how much revenue comes from “loyal” customers to plug into their model. A “loyal” customer is defined as (1) having a membership with your company’s point system, (2) having &gt;2 stays at each hotel the customer visited, (3) having stayed at 3 different locations throughout the US. You have a table showing all transactions made in 2017. The schema of the table is below: customer_transactions Column Name Data Type Description customer_id id id of the customer hotel_id integer unique id for hotel transaction_id integer id of the given transaction first_night string first night of the stay, column format is “YYYY-mm-dd” number_of_nights integer # of nights the customer stayed in hotel total_spend integer total spend for transaction, in USD is_member boolean indicates if the customer is a member of our points system Given this, can you write a SQL query that calculates percent of revenue loyal customers brought in 2017? WITH plus_two_hotel AS ( SELECT customer_id, hotel_id, COUNT(*) AS two_plus_at_hotel FROM customer_transactions GROUP BY customer_id, hotel_id HAVING COUNT(*) &gt; 2 ), three_locations AS ( SELECT customer_id, DISTINCT(hotel_id), COUNT(*) AS three_hotels FROM customer_transactions GROUP BY customer_id, DISTINCT(hotel_id) HAVING COUNT(*) &gt; 3 ), joined AS ( SELECT customer_id, ) "],["python-r-analyses.html", "Chapter 3 Python / R analyses 3.1 Sample Questions", " Chapter 3 Python / R analyses 3.1 Sample Questions 3.1.1 Active users on a messaging application Data Analysis Python Pandas Data Manipulation External Dataset Here is a table schema for a P2P messaging application. The table contains send/receive message data for the application’s users. The structure is as follows: Table name: user_messaging user_messaging date sender_id (#id of the message sender) receiver_id (#id of the message receiver) Using Python, calculate what fraction of senders sent messages to at least 9 unique people on March 1, 2018. Click here to view this problem in an interactive Colab (Jupyter) notebook. # In Python import pandas as pd data = pd.read_csv(&#39;https://raw.githubusercontent.com/erood/interviewqs.com_code_snippets/master/Datasets/ddi_message_app_data.csv&#39;) print(data.head()) ## date sender_id receiver_id ## 0 2018-03-01 5 2 ## 1 2018-03-01 8 6 ## 2 2018-03-01 1 2 ## 3 2018-03-01 4 8 ## 4 2018-03-01 2 7 print(data.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 999 entries, 0 to 998 ## Data columns (total 3 columns): ## date 999 non-null object ## sender_id 999 non-null int64 ## receiver_id 999 non-null int64 ## dtypes: int64(2), object(1) ## memory usage: 23.5+ KB ## None def fraction_sent_by_day(data:pd.DataFrame, day:str, count:int) -&gt; int: df_filtered = data[data[&#39;date&#39;] == day] df_filtered = df_filtered.drop_duplicates(subset = [&#39;sender_id&#39;, &#39;receiver_id&#39;], keep = &#39;first&#39;).reset_index(drop=True) numerator = len(df_filtered) return numerator / len(data) fraction_sent_by_day(data, &#39;2018-03-01&#39;, 9) ## 0.0890890890890891 # In R library(tidyverse) df_raw = read.csv(&#39;https://raw.githubusercontent.com/erood/interviewqs.com_code_snippets/master/Datasets/ddi_message_app_data.csv&#39;) df &lt;- df_raw %&gt;% filter(date == &#39;2018-03-01&#39;) %&gt;% group_by(sender_id) %&gt;% summarise(unique_sends = n_distinct(receiver_id)) %&gt;% summarise(answer = sum(unique_sends &gt;= 9) / n()) df[1] ## # A tibble: 1 × 1 ## answer ## &lt;dbl&gt; ## 1 0.9 3.1.2 Time for a response on a messaging application Data Analysis Python Pandas Data Manipulation You are given a dataset with information around messages sent between users in a P2P messaging application. Below is the dataset’s schema: Column Name Data Type Description date string date of the message sent/received, format is ‘YYYY-mm-dd’ timestamp integer timestamp of the message sent/received, epoch seconds sender_id integer id of the message sender receiver_id integer id of the message receiver Given this, write code to find the fraction of messages that are sent between the same sender and receiver within five minutes (e.g. the fraction of messages that receive a response within 5 minutes). # group by sender and receiver import pandas as pd df = pd.read_csv(&#39;data/data_analysis_2-messages.csv&#39;) df.head() ## date timestamp sender_id receiver_id ## 0 2018-03-01 1519923378 1 5 ## 1 2018-03-01 1519942810 1 4 ## 2 2018-03-01 1519918950 1 5 ## 3 2018-03-01 1519930114 1 2 ## 4 2018-03-01 1519920410 1 2 3.1.3 Cleaning and analyzing employee data Data Analysis Pandas Data Manipulation Data Cleaning Below is a snippet from a table that contains information about employees that work at Company XYZ: employee_name employee_id date_joined age yrs_of_experience Andy 123456 2015-02-15 45 24 Beth 789456 NaN 36 15 Cindy 654123 2017-05-16 34 14 Dale 963852 2018-01-15 25 4 Company XYZ recently migrated database systems causing some of the date_joined records to be NULL. You’re told by an analyst in human resources NULL records for the date_joined field indicates the employees joined prior to 2010. You also find out there are multiple employees with the same name and duplicate records for some employees. Given this, write code to find the number of employees that joined each month. You can group all of the null values as Dec 1, 2009. # in R library(tidyverse) library(lubridate) df &lt;- data.frame(employee_name = c(&quot;Andy&quot;,&quot;Beth&quot;, &quot;Cindy&quot;, &quot;Dale&quot;, &quot;Andy&quot;), emloyee_id = c(123456, 789456, 654123, 963852, 123222), date_joined = c(&quot;2015-02-15&quot;, NA, &quot;2017-05-16&quot;, &quot;2018-01-15&quot;, &quot;2018-01-15&quot;), age = c(45, 36, 34, 25, 45), yrs_of_experience = c(24, 15, 14, 4, 24)) df %&gt;% replace_na(list(date_joined=&quot;2009-12-01&quot;)) %&gt;% mutate(yr_month = floor_date(as_date(date_joined), &quot;month&quot;)) %&gt;% group_by(employee_name, age, yrs_of_experience) %&gt;% mutate(dupe = n() &gt; 1) %&gt;% slice(1) %&gt;% ungroup() %&gt;% group_by(yr_month) %&gt;% summarize(n = n()) %&gt;% arrange(yr_month) ## # A tibble: 4 × 2 ## yr_month n ## &lt;date&gt; &lt;int&gt; ## 1 2009-12-01 1 ## 2 2015-02-01 1 ## 3 2017-05-01 1 ## 4 2018-01-01 1 3.1.4 Analyzing employee data Below is a snippet from a table that contains information about employees that work at Company XYZ: Column name Data type Example value Description employee_name string Cindy Name of employee employee_id integer 1837204 Unique id for each employee yrs_of_experience integer 14 total working years of experience yrs_at_company integer 10 total working years at Company XYZ compensation integer 100000 dollar value of employee compensation career_track string technical Potential values: technical, non-technical, executive Company XYZ Human Resource department is trying to understand compensation across the company and asked you to pull data to help them make a decision regarding employee compensation. Question: Can you pull the average, median, minimum, maximum, and standard deviations for salary across 5 year experience buckets at Company XYZ? (e.g. get the corresponding average, median, minimum, maximum, and standard deviations for experience buckets 0-5, 5-10, 10-15, etc.) df &lt;- data.frame(yrs_of_experience = c(sample(1:25, 100, replace=T))) df %&gt;% mutate(age_cat = case_when( yrs_of_experience &lt; 5 ~ &quot;0-5&quot;, yrs_of_experience &gt;= 5 &amp; yrs_of_experience &lt; 10 ~ &quot;5-10&quot;, yrs_of_experience &gt;= 10 &amp; yrs_of_experience &lt; 15 ~ &#39;10-15&#39;, yrs_of_experience &gt;= 15 &amp; yrs_of_experience &lt; 20 ~ &#39;15-20&#39;, TRUE ~ &quot;Over 20&quot;)) %&gt;% group_by(age_cat) %&gt;% summarize(average= mean(yrs_of_experience), median = median(yrs_of_experience), min = min(yrs_of_experience), max = max(yrs_of_experience), sd = sd(yrs_of_experience)) ## # A tibble: 5 × 6 ## age_cat average median min max sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0-5 2.32 2 1 4 1.11 ## 2 10-15 11.9 12 10 14 1.36 ## 3 15-20 17.3 17 15 19 1.65 ## 4 5-10 6.81 6 5 9 1.66 ## 5 Over 20 22.3 22 20 25 1.67 "],["statistics-and-machine-learning.html", "Chapter 4 Statistics and Machine Learning 4.1 Overview of Important Concepts 4.2 Sample Questions", " Chapter 4 Statistics and Machine Learning 4.1 Overview of Important Concepts 4.1.1 Probability and Sampling Distributions Below is a review of conditional probabilities, Bayes’ theorem, and central limit theorem and how to handle questions that work with commonly referenced probability distributions source 4.1.1.1 Conditional Probabilities &amp; Bayes Theorem In conditional probabilities, we want to figure out the probability of something happening, given that we have some additional information that may influence the outcome. Figure 4.1: There is an overlap between sets A and B, which represents the intersection of both sets (and the probability of both independent events occurring). BAYES THEOREM Bayes theorem is a staple in Data Science interviews. Bayes’ theorem helps us tackle probability questions where we already know about the probability of B given A, but we want to find the probability of A given B. Above we are solving for A given B by multiplying the independent events in the numerator to get the probability of A and B occurring together. We then divide the by probability of B to get the answer. Another way to solve these questions is through tree diagrams. Since given a sequence of independent events, you can chain together the singular probabilities to compute the overall probability. Example 1 : What is the probability that the applicant passes the stats interview, given that he or she passes the coding interview as well? make tree diagram multiply the independent events then compute the probability of each outcome plug into Bayes theorem #passes both the stats and coding interviews both = 0.25 * 0.40 #0.1 #fails stats and passes the coding interview coding = (0.25 * 0.40) + (0.75 * 0.20) #0.25 stats_given_coding = both / coding print(stats_given_coding) ## 0.4 Therefore, there is a 40% chance of passing the the stats interview, given that they passed the coding interview. Example 2: You have two coins in your hand. Out of the two coins, one is a real coin (heads and tails) and the other is a faulty coin with tails on both sides. You are blindfolded and forced to choose a random coin and then toss it in the air. The coin lands with tails facing upwards. Find the probability that this is the faulty coin. # (tails) # P(faulty) # P(tails and faulty) # Print P(faulty | tails) print(3 / 4) ## 0.75 print(1 / 2) ## 0.5 print((1 / 2) * 1) ## 0.5 print((0.5*1)/(0.5*0.5+0.5*1)) ## 0.6666666666666666 Figure 4.2: tree diagram for faultly coin 4.1.1.2 Central limit theorem What - Central limit theorem says that with a large enough collection of samples from the same population, the sample means will be normally distributed. Note that this doesn’t make any assumptions about the underlying distribution of the data; with a reasonably large sample of roughly 30 or more, this theorem will always ring true no matter what the population looks like. Why - Central limit theorem matters because it promises our sampling mean distribution will be normal, therefore we can perform hypothesis tests. More concretely, we can assess the likelihood that a given mean came from a particular distribution and then, based on this, reject or fail to reject our hypothesis. This empowers all of the A/B testing you see in practice. CLT vs law of large numbers - The law of large numbers states that as the size of a sample is increased, the estimate of the sample mean will more accurately reflect the population mean. This is different from the central limit theorem which is more broadly about normality vs sampling. law of large numbers example: from numpy.random import randint #randint is exclusive for high number (second parameter) small = randint(1, 7, 10) small_mean = sum(small) / len(small) large = randint(1, 7, 1000) large_mean = sum(large) / len(large) print(small_mean) ## 3.3 print(large_mean) ## 3.494 Notice how the mean of the large sample has gotten closer to the true expected mean value of 3.5 for a rolled die. CLT example: from numpy.random import randint import matplotlib.pyplot as plt # generating samples means = [randint(1, 7, 30).mean() for i in range(100)] plt.hist(means) ## (array([ 1., 0., 5., 7., 12., 26., 31., 10., 6., 2.]), array([2.26666667, 2.48333333, 2.7 , 2.91666667, 3.13333333, ## 3.35 , 3.56666667, 3.78333333, 4. , 4.21666667, ## 4.43333333]), &lt;a list of 10 Patch objects&gt;) plt.show() 4.1.1.3 Probabilty distributions Probability distributions are to statistics what data structures are to computer science. Simple description - they indicate the likelihood of an outcome. Properties probabilities must add up to 1 Figure 4.3: Area under the curve (AUC) equals 1. Types Bernoulli Binomial Poisson Normal(Gaussian) Others: Uniform, hypergeometric, log normal, student’s t, chi-squared, gamma, beta, webull, exponential, geometric, negative binomial 4.1.1.3.1 Bernoulli distribution Bernoulli is a discrete distribution that models the probability of two outcomes (e.g. a coin flip). There only two possible outcomes, and the probability of one is always 1- &lt;the_other&gt;. #simulating bernoulli data with scipy stats from scipy.stats import bernoulli data = bernoulli.rvs(p=0.5, size=1000) plt.hist(data) ## (array([495., 0., 0., 0., 0., 0., 0., 0., 0., 505.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), &lt;a list of 10 Patch objects&gt;) plt.show() When using a small sample size heads and tails don’t have the exact same probability. This is no fluke — when sampling, we won’t always get perfect results. We can increase our accuracy however, when you increase the size of the sample. 4.1.1.3.2 Binomial distribution The Binomial distribution can be thought of as the sum of the outcomes of multiple Bernoulli trials, meaning those that have an established success and failure. It’s used to model the number of successful outcomes in trials where there is some consistent probability of success. Parameters k - number of successes n - number of trials p - probability of success Motivating example: Consider a game where you are trying to make a ball in a basket. You are given 10 shots and you know that you have an 80% chance of making a given shot. #simulating binomial data with scipy stats from scipy.stats import binom data = binom.rvs(n=10, p=0.80, size=1000) plt.hist(data) ## (array([ 2., 6., 26., 0., 85., 197., 0., 294., 285., 105.]), array([ 3. , 3.7, 4.4, 5.1, 5.8, 6.5, 7.2, 7.9, 8.6, 9.3, 10. ]), &lt;a list of 10 Patch objects&gt;) plt.show() # Probability of making 8 or less shots out of 10 prob1 = binom.cdf(k=8, n=10, p=0.8) # Probability of making 10 out of 10 shots prob2 = binom.pmf(k=10, n=10, p=0.8) print(prob1) ## 0.6241903616 print(prob2) ## 0.10737418240000005 Remember, interviewers like to start out with fundamental concepts before getting incrementally more complex. Above it started with just showing the general shape of the distribution but went into application. 4.1.1.3.3 Normal (Gaussian) distribution The normal distribution is a bell-curve shaped continuous probability distribution that is fundamental to many statistical concepts, like sampling and hypothesis testing. Figure 4.4: 68-95-99.7 rule, aka 68% of observations fall within 1 std, 95% of observations fall within 2 std and so on. # Generate normal data from scipy.stats import norm data = norm.rvs(size=1000) plt.hist(data) ## (array([ 7., 28., 101., 156., 249., 215., 156., 62., 20., 6.]), array([-2.90245849, -2.3031312 , -1.7038039 , -1.10447661, -0.50514932, ## 0.09417798, 0.69350527, 1.29283256, 1.89215986, 2.49148715, ## 3.09081444]), &lt;a list of 10 Patch objects&gt;) plt.show() # Given a standardized normal distribution, what is the probability #of an observation greater than 2? true_prob = 1 - norm.cdf(2) # Looking at our sample, what is the probability of an observation greater than 2? sample_prob = sum(obs &gt; 2 for obs in data) / len(data) print(true_prob) ## 0.02275013194817921 print(sample_prob) ## 0.022 Note that the results from the true distribution and sample distribution are different. 4.1.1.3.4 Poisson distribution The poisson distribution represents a count or the number of times something happened. Unlike the binomial distribution, it’s calculated by an average rate (lambda) instead of a probability p and number of trials n. As the rate of events change the distribution changes as well. When to use: use the Poisson when counting events over time given some continuous rate. E.g.: In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour? 4.1.2 Exploratory Data Analysis 4.1.2.1 Descriptive Statistics There are two most common questions types in interviews along with some other concepts that don’t go in those categories: measures of centrality Core measures mean - average (sum / # of observations) median - middle value when all observations are sorted mode - most common observation, or peak of the distribution Figure 4.5: Mean, median and mode. Notes: Mean, median and mode are all equal if the distribution is perfectly normal Measures of variability Core measures variance - how far spread out your data points are from each other. standard deviation - how far away your data is from the average. range - max - min Figure 4.6: Formula for variance and standard deviation import math import numpy as np nums = [1, 2, 3, 4, 5] mean = sum(nums) / len(nums) # Compute the variance and print the std of the list variance = sum(pow(x - mean, 2) for x in nums) / len(nums) std = math.sqrt(variance) # Compute and print the actual result from numpy real_std = np.array(nums).std() print(std) ## 1.4142135623730951 print(real_std) ## 1.4142135623730951 Other Modality - multiple peaks that show in the data (bi-modal = two peaks) Skewness - the symmetry of the distribution (skewness is determined by where the the tail is) Figure 4.7: Left skewed data 4.1.2.2 Categorical Data Types of categorical data: ordinal - takes on order (e.g. 5 star movie reviews) nominal - no order (e.g. gender, or eye color) Preprocessing categorical variables unlike continuous data, to use machine learning on categorical data you need to encode it. Types of encoding Label encoding - simply assigning a number to the text factor of the category One Hot encoding - break every subcategory into its own Boolean column from sklearn import preprocessing #What data looks like: print(laptops[:5].to_numpy().tolist()) # Encoder - check out new values ## [[&#39;Acer&#39;, &#39;Aspire 3&#39;, 400.0], [&#39;Asus&#39;, &#39;Vivobook E200HA&#39;, 191.9], [&#39;Asus&#39;, &#39;E402WA-GA010T (E2-6110/2GB/32GB/W10)&#39;, 199.0], [&#39;Asus&#39;, &#39;X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux)&#39;, 389.0], [&#39;Asus&#39;, &#39;X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce&#39;, 522.99]] encoder = preprocessing.LabelEncoder() company_column_encoded = encoder.fit_transform(laptops[&#39;Company&#39;]) print(company_column_encoded) ## [0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 2 1 ## 1 0 1 2 1 0 0 0 0 2 0 2 0 0 0 1 2 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 2 0 0 ## 0 1 0 1 0 1 1 1 2 0 1 0 1 0 0 1 2 1 1 1 1 2 1 1 0 1] # One-hot encode Company for laptops2 laptops2 = pd.get_dummies(data=laptops, columns=[&#39;Company&#39;]) print(laptops2.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 100 entries, 0 to 99 ## Data columns (total 5 columns): ## Product 100 non-null object ## Price 100 non-null float64 ## Company_Acer 100 non-null uint8 ## Company_Asus 100 non-null uint8 ## Company_Toshiba 100 non-null uint8 ## dtypes: float64(1), object(1), uint8(3) ## memory usage: 2.0+ KB ## None Be careful about the high dimensionality of using one-hot-encoding! import seaborn as sns from matplotlib import pyplot as plt sns.countplot(laptops[&#39;Company&#39;]) plt.show() # Visualize the relationship with price from matplotlib.pyplot import figure figure(figsize=(3, 3), dpi=80) laptops.boxplot(&#39;Price&#39;, &#39;Company&#39;,rot=30) plt.show() 4.1.2.3 Two or more variables Overview: Comparing the relationships between two or more numerical variables including correlation, confidence intervals and more. Correlation - describes the relatedness between variables, meaning how much information variables reveal about each other. Figure 4.8: Correlation with various scatterplots Covariance - the average of the product between the values of each sample where the values have each had their mean subtracted. difficult to interpret since don’t get a magnitude, but we can use it to get the Pearsons correlation coefficient Pearson’s correlation coefficent - denoted as lowercase r, is the covariance function divided by the product of the sample standard deviations of each variable. Figure 4.9: Positive r means there is a positive relationship, and vise versa. 1 (or -1) is perfect, 0 is no corelation. R^2 - is simply Pearson’s correlation coefficent squared. Interpreted as the amount of variable Y that is explained by X. # data import hidden - obviously import seaborn as sns from matplotlib import pyplot as plt sns.pairplot(weather) Some bivariate comparisons appear more correlated then others. Lets look at Humidity9am and Humidity3pm closer: from matplotlib.pyplot import figure figure(figsize=(3, 3), dpi=80) plt.scatter(weather[&#39;Humidity9am&#39;], weather[&#39;Humidity3pm&#39;]) plt.show() # corr is built into pandas r = weather[&#39;Humidity9am&#39;].corr(weather[&#39;Humidity3pm&#39;]) # Calculate the r-squared value and print the result r2 = r * r print(r2) ## 0.446620063530763 We see here that humidity in the morning has a moderately strong correlation with afternoon humidity, giving us a ~0.67 pearson coefficient. When we square that result, we get a r-squared value of ~0.45, meaning that Humidity9am explains around 45% of the variability in the Humidity3pm variable. Not so robust to outliers When outliers are present the metric is not very robust. So often you must seek other tools when the outliers can’t or shouldn’t be easily removed. Alternatives: Spearman’s/Kendall Things to consider before removing an outlier: If you’re actually building a predictive model, do you have additional data on which to test your model for predictive power? If yes, then you can see what happens when you remove this value. Does removing it improve your ability to make predictions on new data? Then maybe it really is an outlier. Or is it actually providing some important information on the range of possible but perhaps rare outcomes? Would it be unreasonable to expect to see these kinds of extreme values in new data in the future? Then maybe it’s better to include it, or consider a model that can better capture these kinds of values. Like a robust regression Correlation vs Causation Correlation ≠ causation. Need experimentation (sufficiency | necessity) 4.1.3 Statistical Experiments and Significance Testing Introduction into sampling A sample is a collection of data from a certain population that is meant to represent the whole. However, sampling only makes up a small proportion of the total population. But with statistics we want to make conclusions about the sample and generalize it to a broader group, also known as inference. Figure 4.10: Error types in statistics 4.1.3.1 Confiendence Intervals Confidence interval is a range of values that we are fairly sure includes the true value of an unknown population parameter. It has an associated confidence level that represents the frequency in which the interval will contain this value. E.g. a 95% confidence interval means that 95 times out of 100 we can expect our interval to hold the true parameter value of the population. How to calculate: means - take the sample mean and then subtract the appropriate z-score for your confidence level with the population standard deviation over the square root of the number of samples. Takes on a different form if you don’t know the population variance. proportions - take the mean plus minus the z-score times the square root of the proportion times its inverse, over the number of samples. Both above formulas are alike since they take the mean plus minus some value that is computed. The value is referred to as the margin of error. Adding it to the mean gives us the upper value threshold and subtracting it gives us the lower threshold. How a interviewer will ask you about CI: explain in simple terms elaborate on how they are calculated, and maybe implement it. from scipy.stats import sem, t data = [1, 2, 3, 4, 5] confidence = 0.95 z_score = 2.7764451051977987 sample_mean = 3.0 # Compute the standard error and margin of error std_err = sem(data) margin_error = std_err * z_score # Compute and print the lower threshold lower = sample_mean - margin_error # Compute and print the upper threshold upper = sample_mean + margin_error print(lower) ## 1.036756838522439 print(upper) ## 4.9632431614775605 # Calculate the mean and std mean, std = laptops[&#39;Price&#39;].mean(), laptops[&#39;Price&#39;].std() # Compute and print the upper and lower threshold cut_off = std * 3 lower, upper = mean - cut_off, mean + cut_off print(lower, &#39;to&#39;, upper) # Identify and print rows with outliers ## -824.6236866989851 to 2562.812886698985 outliers = laptops[(laptops[&#39;Price&#39;] &gt; upper) | (laptops[&#39;Price&#39;] &lt; lower)] print(outliers) # Drop the rows from the dataset ## Company Product Price ## 61 Asus ROG G703VI-E5062T 3890.0 ## 65 Asus Rog G701VIK-BA060T 2999.0 laptops = laptops[(laptops[&#39;Price&#39;] &lt;= upper) | (laptops[&#39;Price&#39;] &gt;= lower)] In this scenario, dropping the outliers was likely the right move since the values were unthinkable for laptops prices. This implies that there was some mistake in data entry or data collection. With that being said, this won’t always be the best path forward. It’s important to understand why you got the outliers that you did and if they provide valuable information before you throw them out. Using statsmodels instead. from statsmodels.stats.proportion import proportion_confint from scipy.stats import binom heads = binom.rvs(50, 0.5) confidence_int = proportion_confint(heads, nobs=50, alpha=0.01) print(confidence_int) # Repeat this process 10 times ## (0.4431847495717296, 0.7968152504282704) heads = binom.rvs(50, 0.5, size=10) for val in heads: # for 90% confidence confidence_interval = proportion_confint(val, 50, .10) print(confidence_interval) ## (0.44453174400822454, 0.6754682559917756) ## (0.44453174400822454, 0.6754682559917756) ## (0.3836912846323326, 0.6163087153676674) ## (0.30518968814451874, 0.5348103118554812) ## (0.42406406993539053, 0.6559359300646095) ## (0.3245317440082245, 0.5554682559917755) ## (0.44453174400822454, 0.6754682559917756) ## (0.3440640699353905, 0.5759359300646095) ## (0.3245317440082245, 0.5554682559917755) ## (0.2860411978842442, 0.5139588021157558) In the loop, there might be at least one confidence interval that does not contain 0.5, the true population proportion for a fair coin flip. You could decrease the likelihood of this happening by increasing your confidence level or lowering the alpha value. Motivating Example: partial data from an AB test One-tailed z-test from statsmodels.stats.proportion import proportions_ztest conv_rates = results.groupby(&#39;Group&#39;).mean() #control conversions num_control = results[results[&#39;Group&#39;] == &#39;control&#39;][&#39;Converted&#39;].sum() total_control = len(results[results[&#39;Group&#39;] == &#39;control&#39;]) #treatment conversions num_treat = results[results[&#39;Group&#39;] == &#39;treatment&#39;][&#39;Converted&#39;].sum() total_treat = len(results[results[&#39;Group&#39;] == &#39;treatment&#39;]) count = np.array([num_treat, num_control]) nobs = np.array([total_treat, total_control]) stat, pval = proportions_ztest(count, nobs, alternative=&quot;larger&quot;) print(&#39;{0:0.3f}&#39;.format(pval)) ## 0.897 Two tailed t-test Data - laptops prices = laptops.groupby(&#39;Company&#39;).mean() # Assign the prices of each group asus = laptops[laptops[&#39;Company&#39;] == &#39;Asus&#39;][&#39;Price&#39;] toshiba = laptops[laptops[&#39;Company&#39;] == &#39;Toshiba&#39;][&#39;Price&#39;] # Run the t-test from scipy.stats import ttest_ind tstat, pval = ttest_ind(asus, toshiba) print(&#39;{0:0.3f}&#39;.format(pval)) ## 0.506 4.1.3.2 Hypothesis testing Hypothesis testing is just a means of coming to some statistical inference. So we want to look at the distribution of the data and come to some conculsion about something that we think may or may not be true. Assumptions: Random sampling Independent observations Normally distributed (large enough sample) Constant variance Null equals no effect, alternative represents that the outcome that the treatement does have a conclusive effect Which test to use? The test that is used depends on the situation. If you know the population standard deviation and you have a sufficient sample size - z-test, otherwise t-test. Evaluating results When you run your test, your result will be generated in the form of a test statistic, either a z score or t statistic. Using this, you can compute the p-value, which represents the probability of obtaining the sample results you got, given that the null hypothesis is true. Data - results the same as the CI z-test examples above. import pandas as pd import numpy as np conv_rates = results.groupby(&#39;Group&#39;).mean() print(conv_rates) ## Converted ## Group ## control 0.208333 ## treatment 0.115385 4.1.3.3 Power and sample size Components of a power analysis: effect size (e.g. 20% improvement) significance level / alpha value power - probability of detecting an effect if we see something then we want to have enough power to conclude with high probability that the results are statistically significant. lowering power = increasing chance of a Type II error. from statsmodels.stats.power import zt_ind_solve_power import statsmodels.stats.proportion as prop std_effect = prop.proportion_effectsize(0.20, 0.25) zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=0.05, power=0.80) # 1091.8962 impressions #increase power to 0.95 -&gt; 800 more observations std_effect = prop.proportion_effectsize(0.20, 0.25) zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=0.05, power=0.95) # 1807.76215 Visualizing the relationship between power and sample size from matplotlib.pyplot import figure figure(figsize=(3, 3), dpi=80) sample_sizes = np.array(range(5, 100)) effect_sizes = np.array([0.2, 0.5, 0.8]) from statsmodels.stats.power import TTestIndPower results = TTestIndPower() # Plot the power analysis results.plot_power(dep_var=&#39;nobs&#39;, nobs=sample_sizes, effect_size=effect_sizes) plt.show() Notice that not only does an increase in power result in a larger sample size, but this increase grows exponentially as the minimum effect size is increased. Tools: statsmodel.stats.power zt_ind_solve_power() tt_ind_solve_power() before we use these methods we need to know standardized minimum effect difference - proportion_effectsize() can do this by inputting baseline and desired minimum conversion rates. plt_power function in python to visualize 4.1.3.4 Multiple testing When you run a typical hypothesis test with the significance level set to 0.05, there’s a 5 percent chance that you’ll make a type 1 error and detect an effect that doesn’t exist. The multiple comparisons problem arises when you run several sequential hypothesis tests. Since each test is independent, you can multiply the probability of each type I error to get our combined probability of an error. E.g. 20 hypothesis tests of an associations at a 5% significance makes a 65% chance of at least one error. Figure 4.11: Sequential multiple testing increases likelyhood of error. Common approaches to control for mulitiple comparisons Bonferroni correction Sidak correction Step-based procedures Tukey’s procedure Dunnet’s correction 4.1.3.4.1 Bonferroni correction Bonferroni correction is a common, conservative and an easy approach to perform. You just adjust the p-value as shown below. Side effects: Since the approach is a conservative adjustment, with many tests, the corrected significance level will become very, very small. This reduces power, which means that you are increasingly unlikely to detect a true effect when it occurs. Without Bonferroni Correction Consider a hypothetical situation running 60, 30, and 10 distinct hypothesis tests. Below are the ompute the probability of a Type I error for 60 hypothesis tests with a single-test 5% significance level. error_rate_60 = 1 - (.95**(60)) error_rate_30 = 1 - (.95**(30)) error_rate_10 = 1 - (.95**(10)) print(error_rate_60) ## 0.953930201013048 print(error_rate_30) ## 0.7853612360570628 print(error_rate_10) ## 0.4012630607616213 As you can see, the probability of encountering an error is still extremely high. This is where the Bonferroni correction comes in. While a bit conservative, it controls the family-wise error rate for circumstances like these to avoid the high probability of a Type I error. WITH Bonferroni Correction from statsmodels.sandbox.stats.multicomp import multipletests pvals = [.01, .05, .10, .50, .99] p_adjusted = multipletests(pvals, alpha=0.05, method=&#39;bonferroni&#39;) # Print the resulting conclusions and adjusted p-values themselves print(p_adjusted[0]) ## [ True False False False False] print(p_adjusted[1]) ## [0.05 0.25 0.5 1. 1. ] Above the Bonferroni correction, corrected the family-wise error rate for our 5 hypothesis test results. In the end, only one of the tests remained significant. 4.1.4 Regression and Classification 4.1.4.1 Regression models Regression is a technique used to model and analyze the relationships between variables and how they contribute to producing a particular outcome. In other words, it’s a way to determine which variables have an impact, which factors interact, and how certain we are about those measures. Data: weather data with ,in, max temperature and humidity in morning and afternoon. Linear and logistic regression: 4.1.4.1.1 1. Linear regression Output is continuous #what data looks like print(weather.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 15 entries, 0 to 14 ## Data columns (total 4 columns): ## MinTemp 15 non-null float64 ## MaxTemp 15 non-null float64 ## Humidity9am 15 non-null float64 ## Humidity3pm 15 non-null float64 ## dtypes: float64(4) ## memory usage: 608.0 bytes ## None from sklearn.linear_model import LinearRegression from matplotlib import pyplot as plt from matplotlib.pyplot import figure figure(figsize=(3, 3), dpi=80) # reshape(-1,1) needed to ensure has a index value, which indicates to the #linear model that the data represent a series of observed values for a single #variable. Similar to going wide to long. X_train = np.array(weather[&#39;Humidity9am&#39;]).reshape(-1,1) y_train = weather[&#39;Humidity3pm&#39;] lm = LinearRegression() lm.fit(X_train, y_train) ## LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) preds = lm.predict(X_train) # Assign and print coefficient coef = lm.coef_ print(preds) ## [64.05231955 56.01874907 46.52452942 54.5580999 53.09745072 62.59167037 ## 65.51296873 61.86134578 50.17615236 66.24329332 61.13102119 40.68193271 ## 71.35556544 75.00718838 31.18771306] print(coef) # Plot your fit to visualize your model ## [0.73032459] plt.scatter(X_train, y_train) plt.plot(X_train, preds, color=&#39;red&#39;) plt.show() Despite some noise in the plot, we have a decent looking fit here using Humidity9am to predict the dependent variable Humidity3pm with a linear model. Furthermore, take another look at our coefficient. This means that for every 1 unit of humidity in the morning, we can expect about 0.80 units of humidity in the afternoon. More practically, this information tells us that humidity drops about 20% from morning to afternoon! Evaluating models 4.1.4.1.2 Logistic regression One of the most common machine learning algorithms for two-class classification Output is discrete, which allows us to compute probabilities that each observation belongs to a class, thanks to the sigmoid function. The S-shaped curve takes any real number and maps or converts it between 0 and 1. from sklearn.linear_model import LogisticRegression # Create and fit your model clf = LogisticRegression() clf.fit(X_train, y_train) # Compute and print the accuracy ## LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, ## intercept_scaling=1, l1_ratio=None, max_iter=100, ## multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, ## random_state=None, solver=&#39;warn&#39;, tol=0.0001, verbose=0, ## warm_start=False) ## ## /Users/BigBrother/anaconda/envs/PyData/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. ## FutureWarning) ## /Users/BigBrother/anaconda/envs/PyData/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## y = column_or_1d(y, warn=True) acc = clf.score(X_test, y_test) print(acc) ## 0.7708333333333334 Assumptions: Linear relationship (check with QQplot) Errors to be normally distributed () Homoscedastic - uniform variance (&lt; 4:1 is usually a good rule of thumb) Independence 4.1.4.2 Evalating models Regression: R-squared (coefficient of determination) - tells us the proption of variance of the dependent variable that is explained by the regression model. usually the first metric checked in regression can use score in python MAE (Mean absolute error) - is the sum of the absolute residuals over the number of points MSE (Mean squared error) - is the sum of the residuals squared over the number of points. r2 = lm.score(X, y) from sklearn.metrics import mean_squared_error, mean_absolute_error preds = lm.predict(X) mse = mean_squared_error(y,preds) preds = lm.predict(X) mae = mean_absolute_error(y, preds) print(r2) ## 0.4349683260873261 print(mse) ## 230.88845883712395 print(mae) ## 11.818938818487227 Note that our R-squared value tells us the percentage of the variance of y that X is responsible for. Classification Precision - can be interpreted as the percentage of observations that you correctly guessed and is linked to the rate of type 1 error. Recall - linked to the rate of type II errors. Confusion matrices - shown earlier, but you can use them to determine which error type you would like to prioritize. E.g. A spam detector would probably mean that you don’t want to make any type 1 errors (i.e. don’t want to miss real emails that are marked as spam) and want to optimize for precision. E.g. Classifying a rare disease you want to avoid type II errors (i.e. ), and you would want to optimize for recall. from sklearn.metrics import confusion_matrix, precision_score, recall_score preds = clf.predict(X_test) matrix = confusion_matrix(y_test, preds) print(matrix) ## [[185 0] ## [ 55 0]] precision = precision_score(y_test, preds) #Using precision since you don&#39;t want to say it will rain when it won&#39;t ## /Users/BigBrother/anaconda/envs/PyData/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. ## &#39;precision&#39;, &#39;predicted&#39;, average, warn_for) print(precision) ## 0.0 recall = recall_score(y_test, preds) print(recall) ## 0.0 You can see here that the precision of our rain prediction model was quite high, meaning that we didn’t make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on. 4.1.4.3 Missing data and outliers How to handle missing data: Drop the row - easy but could lose data from your dataset that could strengthen the model df.dropna(inplace=True) Impute the missing values constant value randomly select record value mean, median, mode value estimated by another model (multiple imputation) Dealing with Outliers Standard Deviations - any observations that falls outside of 3 standard deviations from the mean is deemed an outlier. Interquartile range (IQR) - Q1 and Q3 minus (1.5 * IQR). IQR is the difference between Q1 and Q3 nulls = laptops[laptops.isnull().any(axis=1)] print(nulls) ## Empty DataFrame ## Columns: [Company, Product, Price] ## Index: [] laptops.fillna(0, inplace=True) print(laptops.head()) ## Company Product Price ## 0 Acer Aspire 3 400.00 ## 1 Asus Vivobook E200HA 191.90 ## 2 Asus E402WA-GA010T (E2-6110/2GB/32GB/W10) 199.00 ## 3 Asus X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux) 389.00 ## 4 Asus X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce 522.99 laptops.fillna(laptops.Price.median(), inplace=True) print(laptops.head()) ## Company Product Price ## 0 Acer Aspire 3 400.00 ## 1 Asus Vivobook E200HA 191.90 ## 2 Asus E402WA-GA010T (E2-6110/2GB/32GB/W10) 199.00 ## 3 Asus X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux) 389.00 ## 4 Asus X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce 522.99 laptops.dropna(inplace=True) print(laptops.head()) ## Company Product Price ## 0 Acer Aspire 3 400.00 ## 1 Asus Vivobook E200HA 191.90 ## 2 Asus E402WA-GA010T (E2-6110/2GB/32GB/W10) 199.00 ## 3 Asus X540UA-DM186 (i3-6006U/4GB/1TB/FHD/Linux) 389.00 ## 4 Asus X542UQ-GO005 (i5-7200U/8GB/1TB/GeForce 522.99 4.1.4.4 Unbalanced data 5 techniques to handle unbalanced data SMOTE 4.1.4.5 Bias-variance tradeoff Types of error in modeling: Bias error - comes from simplifying assumptions made by a model to make the target function easier to learn. In general, high bias makes algorithms faster to learn and easier to understand but less flexible. Too much bias can lead to a problem with under-fitting the data, which happens when the model is making too many assumptions. High biased algorithms include: linear regression, LDA, and logisitic regression. Figure 4.12: Line is high biased since it is going in a straight flat line when the data isn’t correctly fit, although might be easily trained. Variance error - variance is the amount that the estimate of the target function would change if different training data was used. Some variance will exist but, ideally the results wouldn’t change too much from one training dataset to the next. Too much variance in the model leads to overfitting. It happens when the model is too flexible and fits too closely to the training data, making it not generalizable to unseen data. High variance algorithms include: Decision Trees, k-Nearest Neighbors, and SVM. Figure 4.13: Line is high variance since it too closely follows the data. Irreducible error - won’t focus on. Goal of modeling Figure 4.14: Want to balance model complexity to minimize both bias and variance. 4.2 Sample Questions 4.2.1 Comparison of means So suppose hypothetically Facebook users share, their height in centimeters and their gender. How would you test the hypothesis that men on average are taller? t-test could be used to compare the means of two different groups if the sample is larger then 30, but if larger then 30 we can use a z-test. from statsmodels.stats.weightstats import ztest as ztest women = [82, 84, 85, 89, 91, 91, 92, 94, 99, 99, 82, 84, 85, 89, 91, 91, 92, 94, 99, 99, 105, 109, 109, 109, 110, 112, 112, 113, 114, 114, 105, 109, 109, 109, 110, 112, 112, 113, 114, 114] men = [90, 91, 91, 91, 95, 95, 99, 99, 108, 109, 90, 91, 91, 91, 95, 95, 99, 99, 108, 109, 109, 114, 115, 116, 117, 117, 128, 129, 130, 133, 109, 114, 115, 116, 117, 117, 128, 129, 130, 133] #perform two sample z-test ztest(women, men, value=0) #(-1.9953236073282115, 0.046007596761332065) ## (-2.8587017261290355, 0.004253785496952403) The test statistic for the two sample z-test is -1.9953 and the corresponding p-value is 0.0460. Since this p-value is less than .05, we have sufficient evidence to reject the null hypothesis. In other words, the mean height of men is significantly different vs women. import numpy as np women = [82, 84, 85, 89, 91, 91, 92, 94, 99, 99, 105, 109, 109, 109, 110, 112, 112, 113, 114, 114] men = [90, 91, 91, 91, 95, 95, 99, 99, 108, 109, 109, 114, 115, 116, 117, 117, 128, 129, 130, 133] #check for equal variances, we can assume the populations have equal variances #if the ratio of the larger sample variance to the smaller sample variance #is less than 4:1. print(np.var(women), np.var(men)) ## 119.92750000000001 197.06 import scipy.stats as stats #perform two sample t-test with equal variances stats.ttest_ind(a=women, b=men, equal_var=True) ## Ttest_indResult(statistic=-1.9953236073282115, pvalue=0.05321388037191098) Because the p-value of our test (0.5321) is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height of men vs women is different. 4.2.2 Principle of Inclusion/Exclusion The carshare dilemma * Statistics * Probability Theory Suppose we have selected a group of people to take a survey. 35% of the group like Uber, 20% like both Lyft and Uber, and 25% like neither Lyft nor Uber. Given this information, what percentage of the sample likes Lyft? Hint: You can use basic probability theory to solve this problem. Tip 1: Tip 2: Figure 4.15: carshare dilema 4.2.3 Bayes Theorem 1 Probability of passing through interview stages Statistics Bayes Theorem Given the information below, if you had a good first interview, what is the probability you will receive a second interview? 50% of all people who received a first interview received a second interview 95% of people that received a second interview had a good first interview 75% of people that did not receive a second interview had a good first interview Solution borrowed from here Other good source on Bayes Theorem Figure 4.16: Interview Bayes Theorem 4.2.4 MAE vs MSE Statistics Model Evaluation What are some of the differences you would expect in a model that minimizes squared error, vs a model that minimizes absolute error? In which cases would each error metric be appropriate? Typically, if the dataset has outliers or you’re worried about individual observations, you’ll want to use MSE, since by squaring the errors, they are weighted more heavily. If you aren’t concerned with outliers or single observations then MAE can be used to suppress those errors a bit more, since the absolute values is taken and not the square. 4.2.5 Bias variance tradeoff Statisitics Data modeing In terms of the bias-variance tradeoff, which of the following is substantially more harmful to the test error than the training error? High variance results in overfitting to your training set. You’ll see strong performance at first, until you apply your model to your test set, where it will fail to generalize and likely struggle. 4.2.6 Changing the company’s color Statistics Hypothesis Testing Suppose you work for an eCommerce company that sells cookies. The design team changed the company’s logo color from blue to red, which caused many downstream changes to the company’s landing page, ad creatives, and product packaging. The team did not did not perform an A/B test prior to the change. The lead designer wants to understand if the new color has impacted sales. Explain how you would design an analysis to understand sales impact. 4.2.7 Repeated significance testing error Statistics Communication Context: Your colleagues will often come to you with methodology questions. Please answer the following questions as concisely as possible. Imagine you are responding via email and the person receiving your email is non-technical. 1.1 Your colleague started an A/B test 7 days ago, and they have been checking the p-value every day using an A/B test calculator. Today it finally dipped to 0.04, and they are very excited to move forward with the winner. What recommendations would you make to your non-technical colleague, considering the reported false positive rate? Why? The colleague made a mistake by not following the parameters set at the start of the A/B test. The significance calculation (p-value here) makes a critical assumption that you have probably violated without even realizing it: that the sample size was fixed in advance. If instead of deciding ahead of time, “this experiment will collect exactly 1,000 observations,” you say, “we’ll run it until we see a significant difference,” all the reported significance levels become meaningless. A response could include the fact that a day later the p-value could again dip above the significance test, but because we used a power calculation before hand we need to stick with our original parameters or we risk getting a false positive and could lead to a unexpected decrease in what the test was originally designed to improve. Reference 4.2.8 Bias Variance Tradeoff Can you explain what the Bias-Variance Tradeoff is? Can you share an example of when you might want to use a biased estimator? Think about this in the context of machine learning and training data. Bias-variance tradeoff is related to ML and training data by balancing model complexity. A model that has a high bias might lead to underfitting, where the model makes too many assumptions and are very quick to train. An example of when you would use a biased estimator would be when you need to make a model quickly. In some businesses a linear model might be the right model due to its easily interpreted coefficients and how quickly it can be ran. On the other hand variance is generally associated with over fitting. 4.2.9 Drawing cards from a standard deck Statistics Probability Theory Suppose we have a standard (52 card) deck of cards. Each card has an equal chance of being one of the four “suits” (e.g. clubs, spades, diamonds, and hearts). Question: If you draw 3 cards from the deck, one at a time, what is the probability that you draw a club, a heart, and a diamond in that order? Another example (13/52) x (13/51) x (13/50) = 0.016568627 "],["case-studies.html", "Chapter 5 Case Studies 5.1 Sample Questions", " Chapter 5 Case Studies 5.1 Sample Questions 5.1.1 AB Testing Basics 1 What are some standard statistics that are used in AB Testing. What are the alternative tests and in what situations would you use them? A/B testing is an experimental design, and in its simplest sense is an experiment on two variants to see which performs better based on a given metric. The null hypothesis would be that the effect of condition A is no different than the effect of condition B. Examples of a statistical test used in A/B designs are: Power analysis t-test (for difference in means, like sales) Alternative: Analysis of variance for three or more means. z-test for large sample size (&gt;30 samples) Explaining the t-test - a t-test allows you to determine if average values of two data sets come from the same population (e.g. if sample from placebo and treatment you would expect different different mean and standard deviation) Assumptions scale of measurement applied to the data collected follows a continuous or ordinal scale, like scored on a IQ test. Sample is a simple random sample, representative. and randomly selected. data, when plotted shows a normal distribution. Homogeneity of variance. Happens when standard deviations of samples are approximately equal. Interpreting values: A large t-score indicates that the groups are different. A small t-score indicates that the groups are similar. Figure 5.1: Which T-test to use chi-squared test (for differences in categories like conversions). test of independence this test can tell us how likely it is that a random chance can explain any observed difference between actual frequencies in the data and theoretical expectations. When using A/B testing on boolean variables (click-through rate-click or no click, and conversion-convert or not convert) they follow the discrete probability distribution of Bernoulli Distribution. Example: If showed two different advertisements, A and B to test clickthrough rate for an advertisement. We might have the following information: Click No Click Click + No Click Ad A 360 140 500 Ad B 300 250 550 Ad A + Ad B 660 390 1050 then need to make 4 calculations: Advertisement A, Click observed value = 360 expected value = 500 * (660/1050) = 31.429 Advertisement A, No Click Advertisement B, Click Advertisement B, No Click 1 is done above but each other needs to be calculated and plugged into the formula for the chi squared test statistic. 5.1.2 A/B testing new eCommerce recommendation engine A/B Testing Product Modeling Statistics Suppose you’re working for an eCommerce company like Amazon. Your team is testing a new algorithm to generate recommended products for users. You’ve been tasked with setting up an A/B test to help measure the impact of the change, and decide whether or not to roll out the new recommendation engine more widely. Walk through the steps you would take to set up the A/B test, and highlight some examples of potential pitfalls/risks in your analysis. 5.1.3 A/B testing a new landing page Python Product Data Structures Pandas Data Analysis Suppose you are working for an e-commerce company and the marketing team is trying to decide if they should launch a new webpage. They ran an A/B test and need help analyzing the results. They provided you with this dataset, which contains the following fields: user_id: the user_id of the person visiting the website timestamp: the time in which the user visited the website group: treatment vs control, treatment saw the new landing page, control saw the old landing page landing_page: new vs old landing page, labeled ‘new_page’/‘old_page’ converted: 0/1 flag denoted whether the user visiting the page ended up converting Given this information, you’re asked to come up with a recommendation for the marketing team – should the marketing team adopt the new landing page? The team wants the landing page with the highest conversion rate. 5.1.4 Stopping an AB Test early AB Testing Statistics What are some ways to stop a A/B test early when a fixed population and time have been determined before an A/B Test is implemented normally. Sequential experiment design - Sequential experiment design lets you set up checkpoints in advance where you will decide whether or not to continue the experiment, and it gives you the correct significance levels. Bayesian A/B Testing - With Bayesian experiment design you can stop your experiment at any time and make perfectly valid inferences. Given the real-time nature of web experiments, Bayesian design seems like the way forward. 5.1.5 AB Testing from SQL to modeling "],["computer-science-data-scructures-and-algorithms.html", "Chapter 6 Computer Science / Data Scructures and Algorithms 6.1 Sample Questions", " Chapter 6 Computer Science / Data Scructures and Algorithms 6.1 Sample Questions 6.1.1 Calulate moving average using Python Python Moving Average Data Structures Array You are given a list of numbers J and a single number p. Write a function to return the minimum and maximum averages of the sequences of p numbers in the list J. For example: # Array of numbers J = [4, 4, 4, 9, 10, 11, 12] # Length of sequences, p p = 3 Here, the sequences will be: (4,4,4) (4,4,9) (4,9,10) (9,10,11) (10,11,12) From the above we can see that the minimum average will be 4 and the maximum average will be 11, which corresponds to the first and last sequences. # if always sorted can just do the first 3 and last 3 J = [4, 4, 4, 9, 10, 11, 12] p = 3 def min_max_moving_averages(array, length): min_average = sum(array[0:length])/length max_average = sum(array[0:length])/length for i in range(1, len(array)): average = sum(array[i:i+length])/length if average &lt; min_average: min_average = average if average &gt; max_average: max_average = average return min_average, max_average min_max_moving_averages(J, p) ## (4.0, 11.0) 6.1.2 Python function to express power sets Python Power Set Create a Python function that generates the power set given a set of values. For example, if you’re given the following set: set = {1, 2, 3} Your python function should return the corresponding power set (note this can be a formatted as a list of lists): power set = [[], [1], [2], [1, 2], [3], [1, 3], [2, 3], [1, 2, 3]] 6.1.3 If you were given a m by n matrix how would you find the minimum element using brute force algorithm Python from typing import List #python doesn&#39;t have matrices but a list of a list can be treated as a matrix A = [[1, 4, 5], [-5, 8, 9]] #using min min(min(x) for x in A) ## -5 #brute force def find_min_in_matrix(matrix: List): min_element = matrix[0][0] for sub_list in range(len(matrix)): for element in matrix[sub_list]: if (element &lt; min_element): min_element = element return min_element find_min_in_matrix(A) ## -5 6.1.4 Finding the value closest to 0 Python Data Structures Arrays Suppose you are given a list of Q 1D points. Write code to return the value in Q that is the closest to value j. If two values are equally close to j, return the smaller value. Example: Q = [1, -1, -5, 2, 4, -2, 1] j = 3 Output: 2 Q = [1, -1, -5, 2, 4, -2, 1] Q2 = [1, -1, -5, 2, 4, -2, 1] j = 3 j2 = -4 def closest_value(array, target): differences = {} for i in range(len(array)): differences[array[i]] = target - array[i] min_difference = min([i for i in list(differences.values()) if i &gt;= 0]) return differences.get(min_difference) closest_value(Q, j) ## 2 closest_value(Q2, j2) ## -5 ###Points within an interval Python Data Structures Arrays Suppose you are given P, which is list of j integer intervals, where j is the number of intervals. The intervals are in a format [a, b]. Given an integer z, can you return the number of overlapping intervals for point z? For example: Input: P = [[0, 2], [3, 7], [4, 6], [7, 8], [1 ,5]] z = 5 Output: 3 At z = 5, there are 3 intervals that overlap. The intervals are: [3, 7], [4, 6], and [1, 5] P = [[0, 2], [3, 7], [4, 6], [7, 8], [1 ,5]] z = 5 def num_within_intervals(array, target): count = 0 for i in range(len(array)): if array[i][0] &lt;= target and array[i][1] &gt;= target: count += 1 return count num_within_intervals(P, z) ## 3 ### Checking whether array elements can be made equal #* Python #* Data Structures #* Arrays Given an array a, write a function to feed in the array elements and check whether they can all be made equal by only multiplying the numbers by 2 or 7. (you can multiply by these #s as many times as you like) If all elements can be made equal, return False, otherwise return True. Example: #Input a = [128, 4, 2] #Here, we can turn all elements into 128, by multiplying by 2 #128, 4*2*2*2*2*2 = 128, 2*2*2*2*2*2*2 = 128 #Output: #True #Input a = [65, 4, 2] #Here, we cannot make all elements equal through multiplication by 2 or 7, #so we return false #Output: #False #UNFINISHED # similar - https://www.geeksforgeeks.org/make-all-numbers-of-an-array-equal/ def mult_of_2_or_7(array)-&gt;bool: for i in [1,2]: while array[i] &lt; array[0]: array[i] return True a = [65, 4, 2] b = [128, 4, 2] mult_of_2_or_7(array=a) mult_of_2_or_7(array=b) "]]
