[["index.html", "Data Science Prep Guide Welcome to the Data Science Prep Guide", " Data Science Prep Guide Updated: 2022-05-19 Welcome to the Data Science Prep Guide This book will go over various questions in preparation for Data Scientist interviews. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Data Scientist interviews are tough! Depending on the role that you are interviewing for a interview panel may focus on your technical skills, your business acumen, communications skills and overall fit for the company. This prep guide, is broken up into various sections of a potential interview. 1.0.1 Sections SQL technical questions Python / R analyses Statistics/Machine Learning Case Study Behavioral Questions Computer Science / Data Structures and Algorithms questions "],["sql-questions.html", "Chapter 2 SQL questions 2.1 SQL tips 2.2 Sample Questions", " Chapter 2 SQL questions 2.1 SQL tips 2.2 Sample Questions 2.2.1 Calculate the 30 day readmission rate SQL Database Querying Health records Source: Actual interview question from google. Table: encounter Columns: patient_id, encounter_id, encounter_start, encounter_end, encounter_class (in patient) sql fiddle CREATE TABLE encounter (patient_id int, admid int, adm_date date, dis_date date, encounter_class int) ; INSERT INTO encounter (patient_id, admid, adm_date, dis_date, encounter_class) VALUES (0001, 10000, &#39;2017-01-01 04:05:06&#39;, &#39;2017-01-05 04:05:06&#39;, 5), (0002, 10001, &#39;2017-02-01 04:05:06&#39;, &#39;2017-02-02 04:05:06&#39;, 3), (0003, 10002, &#39;2017-03-01 04:05:06&#39;, &#39;2017-03-04 04:05:06&#39;, 2), (0004, 10003, &#39;2017-04-01 04:05:06&#39;, &#39;2017-04-15 04:05:06&#39;, 4), (0001, 10004, &#39;2017-05-01 04:05:06&#39;, &#39;2017-05-02 04:05:06&#39;, 2), (0005, 10005, &#39;2017-06-01 04:05:06&#39;, &#39;2017-06-05 04:05:06&#39;, 1), (0001, 10006, &#39;2017-04-01 04:05:06&#39;, &#39;2017-04-03 04:05:06&#39;, 1), (0002, 10007, &#39;2017-03-01 04:05:06&#39;, &#39;2017-03-15 04:05:06&#39;, 7), (0006, 10008, &#39;2017-02-01 04:05:06&#39;, &#39;2017-02-05 04:05:06&#39;, 2), (0003, 10009, &#39;2017-07-01 04:05:06&#39;, &#39;2017-07-03 04:05:06&#39;, 1), (0007, 10010, &#39;2017-09-01 04:05:06&#39;, &#39;2017-09-20 04:05:06&#39;, 2) ; /* make indicator for patient id, if readmitted within 30 days merge cte for total count of unique admissions*/ WITH next_date_tbl AS ( SELECT patient_id, adm_date, LEAD(adm_date, 1) OVER ( PARTITION BY patient_id ORDER BY adm_date ) AS next_date FROM encounter ), date_dif_tbl AS ( SELECT patient_id, (CAST(next_date AS DATE) - CAST(adm_date AS DATE)) AS date_dif FROM next_date_tbl ), num_denom AS ( SELECT SUM(CASE WHEN date_dif &lt;= 30 THEN 1 ELSE 0 END) AS readmitted, COUNT(*) AS total_cnt FROM date_dif_tbl ) SELECT (ROUND((readmitted::float/total_cnt::float)::numeric,2) * 100) AS thirty_day_readmission_rt FROM num_denom 2.2.2 Top 10 diagnoses from encounters with a length of service greater then 3 days SQL Database Querying Health records Source: Actual interview question from google. Table: encounter Columns: encounter_id, diagnosis_id, priority, encounter_start, encounter_end sql fiddle CREATE TABLE encounters (encounter_id int PRIMARY KEY, diagnosis_id int, encounter_start TIMESTAMP, encounter_end TIMESTAMP); INSERT INTO encounters (encounter_id, diagnosis_id, encounter_start, encounter_end) VALUES (1, 5, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (2, 5, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (3, 5, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (4, 2, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (5, 2, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;), (6, 3, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-09 04:05:06&#39;), (7, 4, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-09 04:05:06&#39;), (8, 4, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-09 04:05:06&#39;), (9, 1, &#39;2022-01-08 04:05:06&#39;, &#39;2022-01-20 04:05:06&#39;) ; WITH date_calc AS ( SELECT *, (CAST(encounter_end AS date) - CAST(encounter_start AS date)) AS los FROM encounters ), grouped_diagnosis AS ( SELECT diagnosis_id, COUNT(*) AS n FROM date_calc WHERE los &gt; 3 GROUP BY diagnosis_id ) SELECT diagnosis_id FROM grouped_diagnosis ORDER BY n DESC LIMIT 10; 2.2.3 Employee survey results SQL Database Querying Suppose you’re consulting for a company, and you’ve sent out a survey that asks successive qustions randomly. The survey logs data into a table called survey_logging. The schema of the table is: survey_logging Column Name Data Type Description employee_id integer employee id of the survey respondant action string Will be one of the following values, ‘view,’ ‘answer,’ ‘skip’ question_id integer ID of the question asked answer_id integer ID of the answer asked timestamp integer time stamp of the action made by respondant Using SQL, find which question has the highest response rate. db_fiddle /*Intermediate result would look like: | question_id | response_rate | */ CREATE TABLE survey_logging (user_id int PRIMARY KEY, &quot;action&quot; varchar(10), question_id int) ; INSERT INTO survey_logging (user_id, &quot;action&quot;, question_id) VALUES (1, &#39;answer&#39;, 1), (2, &#39;answer&#39;, 1), (3,&#39;answer&#39;, 2), (4,&#39;skip&#39;, 2), (5,&#39;skip&#39;, 1), (6,&#39;answer&#39;, 1), (7,&#39;answer&#39;, 1), (8,&#39;answer&#39;, 1), (9,&#39;skip&#39;, 1), (10,&#39;skip&#39;, 2) ; /*~~~~~~~~~~~~~~~~~~~~~*/ WITH response_by_q AS ( SELECT question_id, SUM (CASE WHEN action = &#39;answer&#39; THEN 1 ELSE 0 END) AS answered, COUNT(*) AS ovr_count FROM survey_logging GROUP BY question_id ), extra_column AS ( SELECT question_id, ROUND(answered * 100.0 / ovr_count, 1) AS answer FROM response_by_q ) SELECT question_id FROM extra_column WHERE answer = (SELECT MAX(answer) FROM extra_column) "],["python-r-analyses.html", "Chapter 3 Python / R analyses 3.1 Sample Questions", " Chapter 3 Python / R analyses 3.1 Sample Questions 3.1.1 Active users on a messaging application Data Analysis Python Pandas Data Manipulation External Dataset Here is a table schema for a P2P messaging application. The table contains send/receive message data for the application’s users. The structure is as follows: Table name: user_messaging user_messaging date sender_id (#id of the message sender) receiver_id (#id of the message receiver) Using Python, calculate what fraction of senders sent messages to at least 9 unique people on March 1, 2018. Click here to view this problem in an interactive Colab (Jupyter) notebook. # In Python import pandas as pd data = pd.read_csv(&#39;https://raw.githubusercontent.com/erood/interviewqs.com_code_snippets/master/Datasets/ddi_message_app_data.csv&#39;) print(data.head()) ## date sender_id receiver_id ## 0 2018-03-01 5 2 ## 1 2018-03-01 8 6 ## 2 2018-03-01 1 2 ## 3 2018-03-01 4 8 ## 4 2018-03-01 2 7 print(data.info()) ## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; ## RangeIndex: 999 entries, 0 to 998 ## Data columns (total 3 columns): ## date 999 non-null object ## sender_id 999 non-null int64 ## receiver_id 999 non-null int64 ## dtypes: int64(2), object(1) ## memory usage: 23.5+ KB ## None def fraction_sent_by_day(data:pd.DataFrame, day:str, count:int) -&gt; int: df_filtered = data[data[&#39;date&#39;] == day] df_filtered = df_filtered.drop_duplicates(subset = [&#39;sender_id&#39;, &#39;receiver_id&#39;], keep = &#39;first&#39;).reset_index(drop=True) numerator = len(df_filtered) return numerator / len(data) fraction_sent_by_day(data, &#39;2018-03-01&#39;, 9) ## 0.0890890890890891 # In R library(tidyverse) df_raw = read.csv(&#39;https://raw.githubusercontent.com/erood/interviewqs.com_code_snippets/master/Datasets/ddi_message_app_data.csv&#39;) df &lt;- df_raw %&gt;% filter(date == &#39;2018-03-01&#39;) %&gt;% group_by(sender_id) %&gt;% summarise(unique_sends = n_distinct(receiver_id)) %&gt;% summarise(answer = sum(unique_sends &gt;= 9) / n()) df[1] ## # A tibble: 1 × 1 ## answer ## &lt;dbl&gt; ## 1 0.9 3.1.2 Time for a response on a messaging application Data Analysis Python Pandas Data Manipulation You are given a dataset with information around messages sent between users in a P2P messaging application. Below is the dataset’s schema: Column Name Data Type Description date string date of the message sent/received, format is ‘YYYY-mm-dd’ timestamp integer timestamp of the message sent/received, epoch seconds sender_id integer id of the message sender receiver_id integer id of the message receiver Given this, write code to find the fraction of messages that are sent between the same sender and receiver within five minutes (e.g. the fraction of messages that receive a response within 5 minutes). # group by sender and receiver import pandas as pd df = pd.read_csv(&#39;data/data_analysis_2-messages.csv&#39;) df.head() ## date timestamp sender_id receiver_id ## 0 2018-03-01 1519923378 1 5 ## 1 2018-03-01 1519942810 1 4 ## 2 2018-03-01 1519918950 1 5 ## 3 2018-03-01 1519930114 1 2 ## 4 2018-03-01 1519920410 1 2 "],["statistics-and-machine-learning.html", "Chapter 4 Statistics and Machine Learning 4.1 Overview of Important Concepts 4.2 Sample Questions", " Chapter 4 Statistics and Machine Learning 4.1 Overview of Important Concepts 4.1.1 Probability and Sampling Distributions Below is a review of conditional probabilities, Bayes’ theorem, and central limit theorem and how to handle questions that work with commonly referenced probability distributions source 4.1.1.1 Conditional Probabilities &amp; Bayes Theorem In conditional probabilities, we want to figure out the probability of something happening, given that we have some additional information that may influence the outcome. Figure 4.1: There is an overlap between sets A and B, which represents the intersection of both sets (and the probability of both independent events occurring). BAYES THEOREM Bayes theorem is a staple in Data Science interviews. Bayes’ theorem helps us tackle probability questions where we already know about the probability of B given A, but we want to find the probability of A given B. Above we are solving for A given B by multiplying the independent events in the numerator to get the probability of A and B occurring together. We then divide the by probability of B to get the answer. Another way to solve these questions is through tree diagrams. Since given a sequence of independent events, you can chain together the singular probabilities to compute the overall probability. Example 1 : What is the probability that the applicant passes the stats interview, given that he or she passes the coding interview as well? make tree diagram multiply the independent events then compute the probability of each outcome plug into Bayes theorem #passes both the stats and coding interviews both = 0.25 * 0.40 #0.1 #fails stats and passes the coding interview coding = (0.25 * 0.40) + (0.75 * 0.20) #0.25 stats_given_coding = both / coding print(stats_given_coding) ## 0.4 Therefore, there is a 40% chance of passing the the stats interview, given that they passed the coding interview. Example 2: You have two coins in your hand. Out of the two coins, one is a real coin (heads and tails) and the other is a faulty coin with tails on both sides. You are blindfolded and forced to choose a random coin and then toss it in the air. The coin lands with tails facing upwards. Find the probability that this is the faulty coin. # (tails) # P(faulty) # P(tails and faulty) # Print P(faulty | tails) print(3 / 4) ## 0.75 print(1 / 2) ## 0.5 print((1 / 2) * 1) ## 0.5 print((0.5*1)/(0.5*0.5+0.5*1)) ## 0.6666666666666666 Figure 4.2: tree diagram for faultly coin 4.1.1.2 Central limit theorem What - Central limit theorem says that with a large enough collection of samples from the same population, the sample means will be normally distributed. Note that this doesn’t make any assumptions about the underlying distribution of the data; with a reasonably large sample of roughly 30 or more, this theorem will always ring true no matter what the population looks like. Why - Central limit theorem matters because it promises our sampling mean distribution will be normal, therefore we can perform hypothesis tests. More concretely, we can assess the likelihood that a given mean came from a particular distribution and then, based on this, reject or fail to reject our hypothesis. This empowers all of the A/B testing you see in practice. CLT vs law of large numbers - The law of large numbers states that as the size of a sample is increased, the estimate of the sample mean will more accurately reflect the population mean. This is different from the central limit theorem which is more broadly about normality vs sampling. law of large numbers example: from numpy.random import randint #randint is exclusive for high number (second parameter) small = randint(1, 7, 10) small_mean = sum(small) / len(small) large = randint(1, 7, 1000) large_mean = sum(large) / len(large) print(small_mean) ## 3.4 print(large_mean) ## 3.495 Notice how the mean of the large sample has gotten closer to the true expected mean value of 3.5 for a rolled die. * CLT example: from numpy.random import randint import matplotlib.pyplot as plt # generating samples means = [randint(1, 7, 30).mean() for i in range(100)] plt.hist(means) ## (array([ 3., 6., 6., 19., 11., 33., 14., 7., 0., 1.]), array([2.63333333, 2.82666667, 3.02 , 3.21333333, 3.40666667, ## 3.6 , 3.79333333, 3.98666667, 4.18 , 4.37333333, ## 4.56666667]), &lt;a list of 10 Patch objects&gt;) plt.show() 4.1.1.3 Probabilty distributions Probability distributions are to statistics what data structures are to computer science. Simple description - they indicate the likelihood of an outcome. Properties probabilities must add up to 1 Figure 4.3: Area under the curve (AUC) equals 1. Types Bernoulli Binomial Poisson Normal(Gaussian) Others: Uniform, hypergeometric, log normal, student’s t, chi-squared, gamma, beta, webull, exponential, geometric, negative binomial 4.1.1.3.1 Bernoulli distribution Bernoulli is a discrete distribution that models the probability of two outcomes (e.g. a coin flip). There only two possible outcomes, and the probability of one is always 1- &lt;the_other&gt;. #simulating bernoulli data with scipy stats from scipy.stats import bernoulli data = bernoulli.rvs(p=0.5, size=1000) plt.hist(data) ## (array([495., 0., 0., 0., 0., 0., 0., 0., 0., 505.]), array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), &lt;a list of 10 Patch objects&gt;) plt.show() When using a small sample size heads and tails don’t have the exact same probability. This is no fluke — when sampling, we won’t always get perfect results. We can increase our accuracy however, when you increase the size of the sample. 4.1.1.3.2 Binomial distribution The Binomial distribution can be thought of as the sum of the outcomes of multiple Bernoulli trials, meaning those that have an established success and failure. It’s used to model the number of successful outcomes in trials where there is some consistent probability of success. Parameters k - number of successes n - number of trials p - probability of success Motivating example: Consider a game where you are trying to make a ball in a basket. You are given 10 shots and you know that you have an 80% chance of making a given shot. #simulating binomial data with scipy stats from scipy.stats import binom data = binom.rvs(n=10, p=0.80, size=1000) plt.hist(data) ## (array([ 1., 1., 26., 0., 70., 205., 0., 304., 296., 97.]), array([ 3. , 3.7, 4.4, 5.1, 5.8, 6.5, 7.2, 7.9, 8.6, 9.3, 10. ]), &lt;a list of 10 Patch objects&gt;) plt.show() # Probability of making 8 or less shots out of 10 prob1 = binom.cdf(k=8, n=10, p=0.8) # Probability of making 10 out of 10 shots prob2 = binom.pmf(k=10, n=10, p=0.8) print(prob1) ## 0.6241903616 print(prob2) ## 0.10737418240000005 Remember, interviewers like to start out with fundamental concepts before getting incrementally more complex. Above it started with just showing the general shape of the distribution but went into application. 4.1.1.3.3 Normal (Gaussian) distribution The normal distribution is a bell-curve shaped continuous probability distribution that is fundamental to many statistical concepts, like sampling and hypothesis testing. Figure 4.4: 68-95-99.7 rule, aka 68% of observations fall within 1 std, 95% of observations fall within 2 std and so on. # Generate normal data from scipy.stats import norm data = norm.rvs(size=1000) plt.hist(data) ## (array([ 8., 34., 106., 195., 277., 224., 111., 40., 4., 1.]), array([-3.16051949, -2.46657265, -1.77262581, -1.07867896, -0.38473212, ## 0.30921472, 1.00316156, 1.6971084 , 2.39105525, 3.08500209, ## 3.77894893]), &lt;a list of 10 Patch objects&gt;) plt.show() # Given a standardized normal distribution, what is the probability of an observation greater than 2? true_prob = 1 - norm.cdf(2) # Looking at our sample, what is the probability of an observation greater than 2? sample_prob = sum(obs &gt; 2 for obs in data) / len(data) print(true_prob) ## 0.02275013194817921 print(sample_prob) ## 0.021 Note that the results from the true distribution and sample distribution are different. 4.1.1.3.4 Poisson distribution The poisson distribution represents a count or the number of times something happened. Unlike the binomial distribution, it’s calculated by an average rate (lambda) instead of a probability p and number of trials n. As the rate of events change the distribution changes as well. When to use: use the Poisson when counting events over time given some continuous rate. E.g.: In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour? 4.1.2 Exploratory Data Analysis 4.1.2.1 Descriptive Statistics There are two most common questions types in interviews along with some other concepts that don’t go in those categories: 1. measures of centrality * Core measures * mean - average (sum / # of observations) * median - middle value when all observations are sorted * mode - most common observation, or peak of the distribution Figure 4.5: Mean, median and mode. Notes: Mean, median and mode are all equal if the distribution is perfectly normal measures of variability Core measures variance - how far spread out your data points are from each other. standard deviation - how far away your data is from the average. range - max - min Figure 4.6: Formula for variance and standard deviation import math import numpy as np nums = [1, 2, 3, 4, 5] mean = sum(nums) / len(nums) # Compute the variance and print the std of the list variance = sum(pow(x - mean, 2) for x in nums) / len(nums) std = math.sqrt(variance) # Compute and print the actual result from numpy real_std = np.array(nums).std() print(std) ## 1.4142135623730951 print(real_std) ## 1.4142135623730951 other modality - multiple peaks that show in the data (bi-modal = two peaks) skewness - the symmetry of the distribution (skewness is determined by where the the tail is) Figure 4.7: Left skewed data 4.1.2.2 Categorical Data Types of categorical data: 1. ordinal - takes on order (e.g. 5 star movie reviews) 2. nominal - no order (e.g. gender, or eye color) Preprocessing categorical variables * unlike continuous data, to use machine learning on categorical data you need to encode it. * Types of encoding 1. Label encoding - simply assigning a number to the text factor of the category 2. One Hot encoding - break every subcategory into its own Boolean column import pandas as pd list_of_laptops = [[&#39;Lenovo&#39;, &#39;ThinkPad 13&#39;, 960.0], [&#39;Apple&#39;, &#39;MacBook Pro&#39;, 1518.55], [&#39;Dell&#39;, &#39;XPS 13&#39;, 1268.0], [&#39;Lenovo&#39;, &#39;ThinkPad Yoga&#39;, 2025.0], [&#39;Lenovo&#39;, &#39;IdeaPad 520S-14IKB&#39;, 599.0], [&#39;Dell&#39;, &#39;Precision 5520&#39;, 2135.0], [&#39;Dell&#39;, &#39;Inspiron 3552&#39;, 379.0], [&#39;Dell&#39;, &#39;Vostro 5568&#39;, 912.5], [&#39;Lenovo&#39;, &#39;ThinkPad X1&#39;, 2339.0], [&#39;Dell&#39;, &#39;Inspiron 5579&#39;, 1049.0]] laptops = pd.DataFrame(list_of_laptops, columns = [&#39;Company&#39;, &#39;Product&#39;, &#39;Price&#39;]) from sklearn import preprocessing # Encoder - check out new values encoder = preprocessing.LabelEncoder() company_column_encoded = encoder.fit_transform(laptops[&#39;Company&#39;]) print(company_column_encoded) ## [2 0 1 2 2 1 1 1 2 1] # One-hot encode Company for laptops2 laptops2 = pd.get_dummies(data=laptops, columns=[&#39;Company&#39;]) print(laptops2.head()) ## Product Price Company_Apple Company_Dell Company_Lenovo ## 0 ThinkPad 13 960.00 0 0 1 ## 1 MacBook Pro 1518.55 1 0 0 ## 2 XPS 13 1268.00 0 1 0 ## 3 ThinkPad Yoga 2025.00 0 0 1 ## 4 IdeaPad 520S-14IKB 599.00 0 0 1 Be careful about the high dimensionality of using one-hot-encoding! import seaborn as sns from matplotlib import pyplot as plt sns.countplot(laptops[&#39;Company&#39;]) plt.show() # Visualize the relationship with price from matplotlib.pyplot import figure figure(figsize=(3, 3), dpi=80) laptops.boxplot(&#39;Price&#39;, &#39;Company&#39;,rot=30) plt.show() 4.1.2.3 Two or more variables Overview: Comparing the relationships between two or more numerical variables including correlation, confidence intervals and more. correlation - describes the relatedness between variables, meaning how much information variables reveal about each other. Figure 4.8: Correlation with various scatterplots Covariance - the average of the product between the values of each sample where the values have each had their mean subtracted. difficult to interpret since don’t get a magnitude, but we can use it to get the Pearsons correlation coefficient Pearson’s correlation coefficent - denoted as lowercase r, is the covariance function divided by the product of the sample standard deviations of each variable. Figure 4.9: Positive r means there is a positive relationship, and vise versa. 1 (or -1) is perfect, 0 is no corelation. R^2 - is simply Pearson’s correlation coefficent squared. Interpreted as the amount of variable Y that is explained by X. # data import hidden - obviously import seaborn as sns from matplotlib import pyplot as plt sns.pairplot(weather) Some appear more correlated then others. Lets look at Humidity9am and Humidity3pm closer: from matplotlib.pyplot import figure figure(figsize=(3, 3), dpi=80) plt.scatter(weather[&#39;Humidity9am&#39;], weather[&#39;Humidity3pm&#39;]) plt.show() Correlation vs Causation * Correlation ≠ causation. * Need experimentation (sufficiency | necessity) 4.1.3 Statistical Experiments and Significance Testing 4.1.3.1 Confiendence Intervals 4.1.3.2 Hypothesis testing 4.1.3.3 Power and sample size Components of a power analysis: * effect size (e.g. 20% improvement) * significance level / alpha value * power - probability of detecting an effect * if we see something then we want to have enough power to conclude with high probability that the results are statistically significant. * lowering power = increasing chance of a Type II error. Figure 4.10: Error types in statistics from statsmodels.stats.power import zt_ind_solve_power import statsmodels.stats.proportion as prop std_effect = prop.proportion_effectsize(0.20, 0.25) zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=0.05, power=0.80) # 1091.8962 impressions #increase power to 0.95 -&gt; 800 more observations std_effect = prop.proportion_effectsize(0.20, 0.25) zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=0.05, power=0.95) # 1807.76215 Tools: * statsmodel.stats.power * zt_ind_solve_power() * tt_ind_solve_power() * before we use these methods we need to know standarized minimum effect difference - proportion_effectsize() can do this by inputting baseline and desired minimum conversion rates. * plt_power function in python to visualize 4.1.3.4 Multiple testing 4.1.4 Regression and Classification 4.1.4.1 Regression models 4.1.4.2 Evalating models 4.1.4.3 Missing data and outliers 4.1.4.4 Unbalanced data https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/ 4.1.4.5 Bias-variance tradeoff 4.2 Sample Questions 4.2.1 Comparison of means So suppose hypothetically Facebook users share, their height in centimeters and their gender. How would you test the hypothesis that men on average are taller? t-test could be used to compare the means of two different groups if the sample is larger then 30, but if larger then 30 we can use a z-test. from statsmodels.stats.weightstats import ztest as ztest women = [82, 84, 85, 89, 91, 91, 92, 94, 99, 99, 82, 84, 85, 89, 91, 91, 92, 94, 99, 99, 105, 109, 109, 109, 110, 112, 112, 113, 114, 114, 105, 109, 109, 109, 110, 112, 112, 113, 114, 114] men = [90, 91, 91, 91, 95, 95, 99, 99, 108, 109, 90, 91, 91, 91, 95, 95, 99, 99, 108, 109, 109, 114, 115, 116, 117, 117, 128, 129, 130, 133, 109, 114, 115, 116, 117, 117, 128, 129, 130, 133] #perform two sample z-test ztest(women, men, value=0) #(-1.9953236073282115, 0.046007596761332065) ## (-2.8587017261290355, 0.004253785496952403) The test statistic for the two sample z-test is -1.9953 and the corresponding p-value is 0.0460. Since this p-value is less than .05, we have sufficient evidence to reject the null hypothesis. In other words, the mean height of men is significantly different vs women. import numpy as np women = [82, 84, 85, 89, 91, 91, 92, 94, 99, 99, 105, 109, 109, 109, 110, 112, 112, 113, 114, 114] men = [90, 91, 91, 91, 95, 95, 99, 99, 108, 109, 109, 114, 115, 116, 117, 117, 128, 129, 130, 133] #check for equal variances, we can assume the populations have equal variances if the ratio of the larger sample variance to the smaller sample variance is less than 4:1. print(np.var(women), np.var(men)) ## 119.92750000000001 197.06 import scipy.stats as stats #perform two sample t-test with equal variances stats.ttest_ind(a=women, b=men, equal_var=True) ## Ttest_indResult(statistic=-1.9953236073282115, pvalue=0.05321388037191098) Because the p-value of our test (0.5321) is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height of men vs women is different. 4.2.2 Principle of Inclusion/Exclusion The carshare dilemma * Statistics * Probability Theory Suppose we have selected a group of people to take a survey. 35% of the group like Uber, 20% like both Lyft and Uber, and 25% like neither Lyft nor Uber. Given this information, what percentage of the sample likes Lyft? Hint: You can use basic probability theory to solve this problem. Tip 1: Tip 2: Figure 4.11: carshare dilema 4.2.3 Bayes Theorem 1 Probability of passing through interview stages * Statistics * Bayes Theorem Given the information below, if you had a good first interview, what is the probability you will receive a second interview? 50% of all people who received a first interview received a second interview 95% of people that received a second interview had a good first interview 75% of people that did not receive a second interview had a good first interview Solution borrowed from here Other good source on Bayes Theorem Figure 4.12: Interview Bayes Theorem 4.2.4 Random ML question 1 How to compare the significance between two confusion matrices? Figure 4.13: How to find significance between two matricies. # If go chi square route two chi-square tests, one for true label = 0 and one for true label = 1. However, I think you should just say your recall/specificity/auc/... was improved some pct. #https://github.com/sepandhaghighi/pycm Use PyCm to calculate the confidence interval of these stats. 95% Confidence Interval of Kappa (0.4934, 0.9291), (0.3687, 0.8612) 95% CI of accuracy (0.80507,0.97271), (0.7571,0.9466) Therefore, two matricies are statistically the same "],["case-studies.html", "Chapter 5 Case Studies 5.1 Sample Questions", " Chapter 5 Case Studies 5.1 Sample Questions 5.1.1 What are some standard statistics that are used in AB Testing. What are the alternative tests and in what situations would you use them? A/B testing is an experimental design, and in its simplest sense is an experiment on two variants to see which performs better based on a given metric. The null hypothesis would be that the effect of condition A is no different than the effect of condition B. Examples of a statistical test used in A/B designs are: Power analysis t-test (for difference in means, like sales) Alternative: Analysis of variance for three or more means. z-test for large sample size (&gt;30 samples) Explaining the t-test - a t-test allows you to determine if average values of two data sets come from the same population (e.g. if sample from placebo and treatment you would expect different different mean and standard deviation) Assumptions scale of measurement applied to the data collected follows a continuous or ordinal scale, like scored on a IQ test. Sample is a simple random sample, representative. and randomly selected. data, when plotted shows a normal distribution. Homogeneity of variance. Happens when standard deviations of samples are approximately equal. Interpreting values: A large t-score indicates that the groups are different. A small t-score indicates that the groups are similar. Figure 5.1: Which T-test to use chi-squared test (for differences in categories like conversions). test of independence this test can tell us how likely it is that a random chance can explain any observed difference between actual frequencies in the data and theoretical expectations. When using A/B testing on boolean variables (click-through rate-click or no click, and conversion-convert or not convert) they follow the discrete probability distribution of Bernoulli Distribution. Example: If showed two different advertisements, A and B to test clickthrough rate for an advertisement. We might have the following information: Click No Click Click + No Click Ad A 360 140 500 Ad B 300 250 550 Ad A + Ad B 660 390 1050 then need to make 4 calculations: Advertisement A, Click observed value = 360 expected value = 500 * (660/1050) = 31.429 Advertisement A, No Click Advertisement B, Click Advertisement B, No Click 1 is done above but each other needs to be calculated and plugged into the formula for the chi squared test statistic. "],["computer-science-data-scructures-and-algorithms.html", "Chapter 6 Computer Science / Data Scructures and Algorithms 6.1 Sample Questions", " Chapter 6 Computer Science / Data Scructures and Algorithms 6.1 Sample Questions 6.1.1 Calulate moving average using Python Python Moving Average Data Structures Array You are given a list of numbers J and a single number p. Write a function to return the minimum and maximum averages of the sequences of p numbers in the list J. For example: # Array of numbers J = [4, 4, 4, 9, 10, 11, 12] # Length of sequences, p p = 3 Here, the sequences will be: (4,4,4) (4,4,9) (4,9,10) (9,10,11) (10,11,12) From the above we can see that the minimum average will be 4 and the maximum average will be 11, which corresponds to the first and last sequences. # if always sorted can just do the first 3 and last 3 J = [4, 4, 4, 9, 10, 11, 12] p = 3 def min_max_moving_averages(array, length): min_average = sum(array[0:length])/length max_average = sum(array[0:length])/length for i in range(1, len(array)): average = sum(array[i:i+length])/length if average &lt; min_average: min_average = average if average &gt; max_average: max_average = average return min_average, max_average min_max_moving_averages(J, p) ## (4.0, 11.0) 6.1.2 Python function to express power sets Python Power Set Create a Python function that generates the power set given a set of values. For example, if you’re given the following set: set = {1, 2, 3} Your python function should return the corresponding power set (note this can be a formatted as a list of lists): power set = [[], [1], [2], [1, 2], [3], [1, 3], [2, 3], [1, 2, 3]] 6.1.3 If you were given a m by n matrix how would you find the minimum element using brute force algorithm Python from typing import List #python doesn&#39;t have matrices but a list of a list can be treated as a matrix A = [[1, 4, 5], [-5, 8, 9]] #using min min(min(x) for x in A) ## -5 #brute force def find_min_in_matrix(matrix: List): min_element = matrix[0][0] for sub_list in range(len(matrix)): for element in matrix[sub_list]: if (element &lt; min_element): min_element = element return min_element find_min_in_matrix(A) ## -5 "]]
